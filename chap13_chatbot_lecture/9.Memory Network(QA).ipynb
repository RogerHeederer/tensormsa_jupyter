{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Network\n",
    "* reference : https://github.com/carpedm20/MemN2N-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 13:51:32) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print (sys.version)\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strory Question and Answer (End to End Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines= [\n",
    "'1 나 는 지금 배가 고프다.',\n",
    "'2 나 는 집에 있다가 왔다.',\n",
    "'3 나 는 지금 판교에 있다.',  \n",
    "'4 너 는 지금 어디 있니?  판교',\n",
    "'5 나 의 회사는 포스코ICT다.',\n",
    "'6 나 는 피자 주문 하고 싶다.',\n",
    "'7 너 는 무엇 을 주문 할려구?  피자'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력값의 명사를 통해 완전한 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def read_data(fname, word2idx, max_words, max_sentences):\n",
    "    # stories[story_ind] = [[sentence1], [sentence2], ..., [sentenceN]]\n",
    "    # questions[question_ind] = {'question': [question], 'answer': [answer], 'story_index': #, 'sentence_index': #}\n",
    "    stories = dict()\n",
    "    questions = dict()\n",
    "    \n",
    "    \n",
    "    if len(word2idx) == 0:\n",
    "        word2idx['<null>'] = 0\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        max_words = max(max_words, len(words))\n",
    "        \n",
    "        # Determine whether the line indicates the start of a new story\n",
    "        if words[0] == '1':\n",
    "            story_ind = len(stories)\n",
    "            sentence_ind = 0\n",
    "            stories[story_ind] = []\n",
    "        \n",
    "        # Determine whether the line is a question or not\n",
    "        if '?' in line:\n",
    "            is_question = True\n",
    "            question_ind = len(questions)\n",
    "            questions[question_ind] = {'question': [], 'answer': [], 'story_index': story_ind, 'sentence_index': sentence_ind}\n",
    "        else:\n",
    "            is_question = False\n",
    "            sentence_ind = len(stories[story_ind])\n",
    "        \n",
    "        # Parse and append the words to appropriate dictionary / Expand word2idx dictionary\n",
    "        sentence_list = []\n",
    "        for k in range(1, len(words)):\n",
    "            w = words[k].lower()\n",
    "            \n",
    "            # Remove punctuation\n",
    "            if ('.' in w) or ('?' in w):\n",
    "                w = w[:-1]\n",
    "            \n",
    "            # Add new word to dictionary\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = len(word2idx)\n",
    "            \n",
    "            # Append sentence to story dict if not question\n",
    "            if not is_question:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '.' in words[k]:\n",
    "                    stories[story_ind].append(sentence_list)\n",
    "                    break\n",
    "            \n",
    "            # Append sentence and answer to question dict if question\n",
    "            else:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '?' in words[k]:\n",
    "                    answer = words[k + 1].lower()\n",
    "                    \n",
    "                    if answer not in word2idx:\n",
    "                        word2idx[answer] = len(word2idx)\n",
    "                    \n",
    "                    questions[question_ind]['question'].extend(sentence_list)\n",
    "                    questions[question_ind]['answer'].append(answer)\n",
    "                    break\n",
    "        \n",
    "        # Update max_sentences\n",
    "        max_sentences = max(max_sentences, sentence_ind+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert the words into indices\n",
    "    for idx, context in stories.items():\n",
    "        for i in range(len(context)):\n",
    "            temp = list(map(word2idx.get, context[i]))\n",
    "            context[i] = temp\n",
    "    \n",
    "    for idx, value in questions.items():\n",
    "        temp1 = list(map(word2idx.get, value['question']))\n",
    "        temp2 = list(map(word2idx.get, value['answer']))\n",
    "        \n",
    "        value['question'] = temp1\n",
    "        value['answer'] = temp2\n",
    "    \n",
    "    return stories, questions, max_words, max_sentences\n",
    "\n",
    "\n",
    "def pad_data(stories, questions, max_words, max_sentences):\n",
    "\n",
    "    # Pad the context into same size with '<null>'\n",
    "    for idx, context in stories.items():\n",
    "        for sentence in context:           \n",
    "            while len(sentence) < max_words:\n",
    "                sentence.append(0)\n",
    "        while len(context) < max_sentences:\n",
    "            context.append([0] * max_words)\n",
    "    \n",
    "    # Pad the question into same size with '<null>'\n",
    "    for idx, value in questions.items():\n",
    "        while len(value['question']) < max_words:\n",
    "            value['question'].append(0)\n",
    "\n",
    "\n",
    "def depad_data(stories, questions):\n",
    "\n",
    "    for idx, context in stories.items():\n",
    "        for i in range(len(context)):\n",
    "            if 0 in context[i]:\n",
    "                if context[i][0] == 0:\n",
    "                    temp = context[:i]\n",
    "                    context = temp\n",
    "                    break\n",
    "                else:\n",
    "                    index = context[i].index(0)\n",
    "                    context[i] = context[i][:index]\n",
    "\n",
    "    for idx, value in questions.items():\n",
    "        if 0 in value['question']:\n",
    "            index = value['question'].index(0)\n",
    "            value['question'] = value['question'][:index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anneal_epoch': 25,\n",
      " 'anneal_rate': 0.5,\n",
      " 'babi_task': 1,\n",
      " 'batch_size': 32,\n",
      " 'checkpoint_dir': './checkpoints',\n",
      " 'data_dir': './bAbI/en-valid',\n",
      " 'edim': 20,\n",
      " 'init_lr': 0.01,\n",
      " 'init_mean': 0.0,\n",
      " 'init_std': 0.1,\n",
      " 'is_test': False,\n",
      " 'lin_start': False,\n",
      " 'max_grad_norm': 40,\n",
      " 'max_sentences': 5,\n",
      " 'max_words': 8,\n",
      " 'mem_size': 50,\n",
      " 'nepoch': 100,\n",
      " 'nhop': 3,\n",
      " 'nwords': 25,\n",
      " 'show_progress': False}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_integer(\"edim\", 20, \"internal state dimension [20]\")\n",
    "flags.DEFINE_integer(\"nhop\", 3, \"number of hops [3]\")\n",
    "flags.DEFINE_integer(\"mem_size\", 50, \"maximum number of sentences that can be encoded into memory [50]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"batch size to use during training [32]\")\n",
    "flags.DEFINE_integer(\"nepoch\", 100, \"number of epoch to use during training [100]\")\n",
    "flags.DEFINE_integer(\"anneal_epoch\", 25, \"anneal the learning rate every <anneal_epoch> epochs [25]\")\n",
    "flags.DEFINE_integer(\"babi_task\", 1, \"index of bAbI task for the network to learn [1]\")\n",
    "flags.DEFINE_float(\"init_lr\", 0.01, \"initial learning rate [0.01]\")\n",
    "flags.DEFINE_float(\"anneal_rate\", 0.5, \"learning rate annealing rate [0.5]\")\n",
    "flags.DEFINE_float(\"init_mean\", 0., \"weight initialization mean [0.]\")\n",
    "flags.DEFINE_float(\"init_std\", 0.1, \"weight initialization std [0.1]\")\n",
    "flags.DEFINE_float(\"max_grad_norm\", 40, \"clip gradients to this norm [40]\")\n",
    "flags.DEFINE_string(\"data_dir\", \"./bAbI/en-valid\", \"dataset directory [./bAbI/en_valid]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"./checkpoints\", \"checkpoint directory [./checkpoints]\")\n",
    "flags.DEFINE_boolean(\"lin_start\", False, \"True for linear start training, False for otherwise [False]\")\n",
    "flags.DEFINE_boolean(\"is_test\", False, \"True for testing, False for training [False]\")\n",
    "flags.DEFINE_boolean(\"show_progress\", False, \"print progress [False]\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "word2idx = {}\n",
    "max_words = 0\n",
    "max_sentences = 0\n",
    "\n",
    "train_stories, train_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "valid_stories, valid_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "test_stories, test_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "\n",
    "pad_data(train_stories, train_questions, max_words, max_sentences)\n",
    "pad_data(valid_stories, valid_questions, max_words, max_sentences)\n",
    "pad_data(test_stories, test_questions, max_words, max_sentences)\n",
    "\n",
    "idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "FLAGS.nwords = len(word2idx)\n",
    "FLAGS.max_words = max_words\n",
    "FLAGS.max_sentences = max_sentences\n",
    "\n",
    "pp.pprint(flags.FLAGS.__flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ProgressBar(Bar):\n",
    "    message = 'Loading'\n",
    "    fill = '#'\n",
    "    suffix = '%(percent).1f%% | ETA: %(eta)ds'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습결과 출력\n",
    "* Memory Network 학습 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class MemN2N(object):\n",
    "    \n",
    "    def __init__(self, config, sess):\n",
    "        self.nwords = config.nwords\n",
    "        self.max_words = config.max_words\n",
    "        self.max_sentences = config.max_sentences\n",
    "        self.init_mean = config.init_mean\n",
    "        self.init_std = config.init_std\n",
    "        self.batch_size = config.batch_size\n",
    "        self.nepoch = config.nepoch\n",
    "        self.anneal_epoch = config.anneal_epoch\n",
    "        self.nhop = config.nhop\n",
    "        self.edim = config.edim\n",
    "        self.mem_size = config.mem_size\n",
    "        self.max_grad_norm = config.max_grad_norm\n",
    "        \n",
    "        self.lin_start = config.lin_start\n",
    "        self.show_progress = config.show_progress\n",
    "        self.is_test = config.is_test\n",
    "\n",
    "        self.checkpoint_dir = config.checkpoint_dir\n",
    "        \n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        \n",
    "        self.query = tf.placeholder(tf.int32, [None, self.max_words], name='input')\n",
    "        self.time = tf.placeholder(tf.int32, [None, self.mem_size], name='time')\n",
    "        self.target = tf.placeholder(tf.float32, [None, self.nwords], name='target')\n",
    "        self.context = tf.placeholder(tf.int32, [None, self.mem_size, self.max_words], name='context')\n",
    "        \n",
    "        self.hid = []\n",
    "        \n",
    "        self.lr = None\n",
    "        \n",
    "        if self.lin_start:\n",
    "            self.current_lr = 0.005\n",
    "        else:\n",
    "            self.current_lr = config.init_lr\n",
    "\n",
    "        self.anneal_rate = config.anneal_rate\n",
    "        self.loss = None\n",
    "        self.optim = None\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.log_loss = []\n",
    "        self.log_perp = []\n",
    "    \n",
    "    def build_memory(self):\n",
    "        self.global_step = tf.Variable(0, name='global_step')\n",
    "        \n",
    "        zeros = tf.constant(0, tf.float32, [1, self.edim])\n",
    "        self.A_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.B_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.C_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        \n",
    "        A = tf.concat([zeros, self.A_], axis=0)\n",
    "        B = tf.concat([zeros, self.B_], axis=0)\n",
    "        C = tf.concat([zeros, self.C_], axis=0)\n",
    "        \n",
    "        self.T_A_ = tf.Variable(tf.random_normal([self.mem_size - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.T_C_ = tf.Variable(tf.random_normal([self.mem_size - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        \n",
    "        T_A = tf.concat([zeros, self.T_A_], axis=0)\n",
    "        T_C = tf.concat([zeros, self.T_C_], axis=0)\n",
    "        \n",
    "        A_ebd = tf.nn.embedding_lookup(A, self.context)   # [batch_size, mem_size, max_length, edim]\n",
    "        A_ebd = tf.reduce_sum(A_ebd, axis=2)              # [batch_size, mem_size, edim]\n",
    "        T_A_ebd = tf.nn.embedding_lookup(T_A, self.time)  # [batch_size, mem_size, edim]\n",
    "        A_in = tf.add(A_ebd, T_A_ebd)                     # [batch_size, mem_size, edim]\n",
    "        \n",
    "        C_ebd = tf.nn.embedding_lookup(C, self.context)   # [batch_size, mem_size, max_length, edim]\n",
    "        C_ebd = tf.reduce_sum(C_ebd, axis=2)              # [batch_size, mem_size, edim]\n",
    "        T_C_ebd = tf.nn.embedding_lookup(T_C, self.time)  # [batch_size, mem_size, edim]\n",
    "        C_in = tf.add(C_ebd, T_C_ebd)                     # [batch_size, mem_size, edim]\n",
    "        \n",
    "        query_ebd = tf.nn.embedding_lookup(B, self.query) # [batch_size, max_length, edim]\n",
    "        query_ebd = tf.reduce_sum(query_ebd, axis=1)      # [batch_size, edim]\n",
    "        self.hid.append(query_ebd)\n",
    "        \n",
    "        for h in range(self.nhop):\n",
    "            q3dim = tf.reshape(self.hid[-1], [-1, 1, self.edim]) # [batch_size, edim] ==> [batch_size, 1, edim]\n",
    "            p3dim = tf.matmul(q3dim, A_in, transpose_b=True)     # [batch_size, 1, edim] X [batch_size, edim, mem_size]\n",
    "            p2dim = tf.reshape(p3dim, [-1, self.mem_size])       # [batch_size, mem_size]\n",
    "            \n",
    "            # If linear start, remove softmax layers\n",
    "            if self.lin_start:\n",
    "                p = p2dim\n",
    "            else:\n",
    "                p = tf.nn.softmax(p2dim)\n",
    "            \n",
    "            p3dim = tf.reshape(p, [-1, 1, self.mem_size]) # [batch_size, 1, mem_size]\n",
    "            o3dim = tf.matmul(p3dim, C_in)                # [batch_size, 1, mem_size] X [batch_size, mem_size, edim]\n",
    "            o2dim = tf.reshape(o3dim, [-1, self.edim])    # [batch_size, edim]\n",
    "            \n",
    "            a = tf.add(o2dim, self.hid[-1]) # [batch_size, edim]\n",
    "            self.hid.append(a)              # [input, a_1, a_2, ..., a_nhop]\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.build_memory()\n",
    "        \n",
    "        self.W = tf.Variable(tf.random_normal([self.edim, self.nwords], mean=self.init_mean, stddev=self.init_std))\n",
    "        a_hat = tf.matmul(self.hid[-1], self.W)\n",
    "        \n",
    "        self.hypothesis = tf.nn.softmax(a_hat)\n",
    "\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=a_hat, labels=self.target)\n",
    "        \n",
    "        self.lr = tf.Variable(self.current_lr)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        \n",
    "        params = [self.A_, self.B_, self.C_, self.T_A_, self.T_C_, self.W]\n",
    "        grads_and_vars = self.opt.compute_gradients(self.loss, params)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], self.max_grad_norm), gv[1]) for gv in grads_and_vars]\n",
    "        \n",
    "        inc = self.global_step.assign_add(1)\n",
    "        with tf.control_dependencies([inc]):\n",
    "            self.optim = self.opt.apply_gradients(clipped_grads_and_vars)\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    def train(self, train_stories, train_questions):\n",
    "        N = int(math.ceil(len(train_questions) / self.batch_size))\n",
    "        cost = 0\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "        \n",
    "        for idx in range(N):\n",
    "            \n",
    "            if self.show_progress:\n",
    "                bar.next()\n",
    "            \n",
    "            if idx == N - 1:\n",
    "                iterations = len(train_questions) - (N - 1) * self.batch_size\n",
    "            else:\n",
    "                iterations = self.batch_size\n",
    "            \n",
    "            query = np.ndarray([iterations, self.max_words], dtype=np.int32)\n",
    "            time = np.zeros([iterations, self.mem_size], dtype=np.int32)\n",
    "            target = np.zeros([iterations, self.nwords], dtype=np.float32)\n",
    "            context = np.ndarray([iterations, self.mem_size, self.max_words], dtype=np.int32)\n",
    "            \n",
    "            for b in range(iterations):\n",
    "                m = idx * self.batch_size + b\n",
    "                \n",
    "                curr_q = train_questions[m]\n",
    "                q_text = curr_q['question']\n",
    "                story_ind = curr_q['story_index']\n",
    "                sent_ind = curr_q['sentence_index']\n",
    "                answer = curr_q['answer'][0]\n",
    "                \n",
    "                curr_s = train_stories[story_ind]\n",
    "                curr_c = curr_s[:sent_ind + 1]\n",
    "\n",
    "                if len(curr_c) >= self.mem_size:\n",
    "                    curr_c = curr_c[-self.mem_size:]\n",
    "                    \n",
    "                    for t in range(self.mem_size):\n",
    "                        time[b, t].fill(t)\n",
    "                else:\n",
    "                    \n",
    "                    for t in range(len(curr_c)):\n",
    "                        time[b, t].fill(t)\n",
    "                    \n",
    "                    while len(curr_c) < self.mem_size:\n",
    "                        curr_c.append([0.] * self.max_words)\n",
    "                \n",
    "                query[b, :] = q_text\n",
    "                target[b, answer] = 1\n",
    "                context[b, :, :] = curr_c\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim, self.loss, self.global_step],\n",
    "                                               feed_dict={self.query: query, self.time: time,\n",
    "                                                          self.target: target, self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar.finish()\n",
    "        \n",
    "        return cost / len(train_questions)\n",
    "    \n",
    "    \n",
    "    def test(self, test_stories, test_questions, label='Test'):\n",
    "        N = int(math.ceil(len(test_questions) / self.batch_size))\n",
    "        cost = 0\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "        \n",
    "        for idx in range(N):\n",
    "            \n",
    "            if self.show_progress:\n",
    "                bar.next()\n",
    "            \n",
    "            if idx == N - 1:\n",
    "                iterations = len(test_questions) - (N - 1) * self.batch_size\n",
    "            else:\n",
    "                iterations = self.batch_size\n",
    "            \n",
    "            query = np.ndarray([iterations, self.max_words], dtype=np.int32)\n",
    "            time = np.zeros([iterations, self.mem_size], dtype=np.int32)\n",
    "            target = np.zeros([iterations, self.nwords], dtype=np.float32)\n",
    "            context = np.ndarray([iterations, self.mem_size, self.max_words], dtype=np.int32)\n",
    "            \n",
    "            for b in range(iterations):\n",
    "                m = idx * self.batch_size + b\n",
    "                \n",
    "                curr_q = test_questions[m]\n",
    "                q_text = curr_q['question']\n",
    "                story_ind = curr_q['story_index']\n",
    "                sent_ind = curr_q['sentence_index']\n",
    "                answer = curr_q['answer'][0]\n",
    "                \n",
    "                curr_s = test_stories[story_ind]\n",
    "                curr_c = curr_s[:sent_ind + 1]\n",
    "                \n",
    "                if len(curr_c) >= self.mem_size:\n",
    "                    curr_c = curr_c[-self.mem_size:]\n",
    "                    \n",
    "                    for t in range(self.mem_size):\n",
    "                        time[b, t].fill(t)\n",
    "                else:\n",
    "                    \n",
    "                    for t in range(len(curr_c)):\n",
    "                        time[b, t].fill(t)\n",
    "                    \n",
    "                    while len(curr_c) < self.mem_size:\n",
    "                        curr_c.append([0.] * self.max_words)\n",
    "                \n",
    "                query[b, :] = q_text\n",
    "                target[b, answer] = 1\n",
    "                context[b, :, :] = curr_c\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim, self.loss, self.global_step],\n",
    "                                               feed_dict={self.query: query, self.time: time,\n",
    "                                                          self.target: target, self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar.finish()\n",
    "        \n",
    "        return cost / len(test_questions)\n",
    "    \n",
    "    \n",
    "    def run(self, train_stories, train_questions, test_stories, test_questions):\n",
    "        if not self.is_test:# add not\n",
    "\n",
    "            for idx in range(self.nepoch):\n",
    "                train_loss = np.sum(self.train(train_stories, train_questions))\n",
    "                test_loss = np.sum(self.test(test_stories, test_questions, label='Validation'))\n",
    "                \n",
    "                self.log_loss.append([train_loss, test_loss])\n",
    "                \n",
    "                state = {\n",
    "                    'loss': train_loss,\n",
    "                    'epoch': idx,\n",
    "                    'learning_rate': self.current_lr,\n",
    "                    'validation_loss': test_loss\n",
    "                }\n",
    "                \n",
    "                print(state)\n",
    "                \n",
    "                \n",
    "                # learning rate annealing\n",
    "                if (not idx == 0) and (idx % self.anneal_epoch == 0):\n",
    "                    self.current_lr = self.current_lr * self.anneal_rate\n",
    "                    self.lr.assign(self.current_lr).eval()\n",
    "            \n",
    "                # If validation loss stops decreasing, insert softmax layers\n",
    "                if idx == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    if self.log_loss[idx][1] > self.log_loss[idx - 1][1]:\n",
    "                        self.lin_start = False\n",
    "\n",
    "                if idx % 10 == 0:\n",
    "                    self.saver.save(self.sess,\n",
    "                                    os.path.join(self.checkpoint_dir, \"MemN2N.model\"),\n",
    "                                    global_step=self.step.astype(int))\n",
    "        else:\n",
    "            self.load()\n",
    "            \n",
    "            valid_loss = np.sum(self.test(train_stories, train_questions, label='Validation'))\n",
    "            test_loss = np.sum(self.test(test_stories, test_questions, label='Test'))\n",
    "            \n",
    "            state = {\n",
    "                'validation_loss': valid_loss,\n",
    "                'test_loss': test_loss\n",
    "            }\n",
    "            \n",
    "            print(state)\n",
    "\n",
    "\n",
    "    def predict(self, test_stories, test_questions):\n",
    "        self.load()\n",
    "\n",
    "        num_instances = len(test_questions)\n",
    "\n",
    "        query = np.ndarray([num_instances, self.max_words], dtype=np.int32)\n",
    "        time = np.zeros([num_instances, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([num_instances, self.nwords], dtype=np.float32)\n",
    "        context = np.ndarray([num_instances, self.mem_size, self.max_words], dtype=np.int32)\n",
    "\n",
    "        for b in range(num_instances):\n",
    "            \n",
    "            curr_q = test_questions[b]\n",
    "            q_text = curr_q['question']\n",
    "            story_ind = curr_q['story_index']\n",
    "            sent_ind = curr_q['sentence_index']\n",
    "            answer = curr_q['answer'][0]\n",
    "            \n",
    "            curr_s = test_stories[story_ind]\n",
    "            curr_c = curr_s[:sent_ind + 1]\n",
    "            \n",
    "            if len(curr_c) >= self.mem_size:\n",
    "                curr_c = curr_c[-self.mem_size:]\n",
    "                \n",
    "                for t in range(self.mem_size):\n",
    "                    time[b, t].fill(t)\n",
    "            else:\n",
    "                \n",
    "                for t in range(len(curr_c)):\n",
    "                    time[b, t].fill(t)\n",
    "                \n",
    "                while len(curr_c) < self.mem_size:\n",
    "                    curr_c.append([0.] * self.max_words)\n",
    "            \n",
    "            query[b, :] = q_text\n",
    "            target[b, answer] = 1\n",
    "            context[b, :, :] = curr_c\n",
    "\n",
    "        predictions = self.sess.run(self.hypothesis, feed_dict={self.query: query, self.time: time, self.context: context})\n",
    "\n",
    "        return predictions, target\n",
    "\n",
    "\n",
    "        \n",
    "    def load(self):\n",
    "        print(' [*] Reading checkpoints...')\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\" [!] No checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1342921257019043, 'epoch': 0, 'learning_rate': 0.01, 'validation_loss': 3.1048181056976318}\n",
      "{'loss': 3.0751833915710449, 'epoch': 1, 'learning_rate': 0.01, 'validation_loss': 3.0453207492828369}\n",
      "{'loss': 3.0151634216308594, 'epoch': 2, 'learning_rate': 0.01, 'validation_loss': 2.9846458435058594}\n",
      "{'loss': 2.9537034034729004, 'epoch': 3, 'learning_rate': 0.01, 'validation_loss': 2.9222722053527832}\n",
      "{'loss': 2.8902895450592041, 'epoch': 4, 'learning_rate': 0.01, 'validation_loss': 2.8576936721801758}\n",
      "{'loss': 2.8244237899780273, 'epoch': 5, 'learning_rate': 0.01, 'validation_loss': 2.7904205322265625}\n",
      "{'loss': 2.7556252479553223, 'epoch': 6, 'learning_rate': 0.01, 'validation_loss': 2.7199821472167969}\n",
      "{'loss': 2.6834366321563721, 'epoch': 7, 'learning_rate': 0.01, 'validation_loss': 2.6459360122680664}\n",
      "{'loss': 2.6074314117431641, 'epoch': 8, 'learning_rate': 0.01, 'validation_loss': 2.567875862121582}\n",
      "{'loss': 2.5272259712219238, 'epoch': 9, 'learning_rate': 0.01, 'validation_loss': 2.4854435920715332}\n",
      "{'loss': 2.4424948692321777, 'epoch': 10, 'learning_rate': 0.01, 'validation_loss': 2.398350715637207}\n",
      "{'loss': 2.3529903888702393, 'epoch': 11, 'learning_rate': 0.01, 'validation_loss': 2.3063983917236328}\n",
      "{'loss': 2.2585690021514893, 'epoch': 12, 'learning_rate': 0.01, 'validation_loss': 2.2095038890838623}\n",
      "{'loss': 2.1592159271240234, 'epoch': 13, 'learning_rate': 0.01, 'validation_loss': 2.1077284812927246}\n",
      "{'loss': 2.0550763607025146, 'epoch': 14, 'learning_rate': 0.01, 'validation_loss': 2.0013060569763184}\n",
      "{'loss': 1.9464766979217529, 'epoch': 15, 'learning_rate': 0.01, 'validation_loss': 1.8906593322753906}\n",
      "{'loss': 1.8339362144470215, 'epoch': 16, 'learning_rate': 0.01, 'validation_loss': 1.7763999700546265}\n",
      "{'loss': 1.7181519269943237, 'epoch': 17, 'learning_rate': 0.01, 'validation_loss': 1.6592977046966553}\n",
      "{'loss': 1.5999457836151123, 'epoch': 18, 'learning_rate': 0.01, 'validation_loss': 1.5402004718780518}\n",
      "{'loss': 1.4801584482192993, 'epoch': 19, 'learning_rate': 0.01, 'validation_loss': 1.4199017286300659}\n",
      "{'loss': 1.3594913482666016, 'epoch': 20, 'learning_rate': 0.01, 'validation_loss': 1.2989627122879028}\n",
      "{'loss': 1.2383224964141846, 'epoch': 21, 'learning_rate': 0.01, 'validation_loss': 1.1775534152984619}\n",
      "{'loss': 1.1166306734085083, 'epoch': 22, 'learning_rate': 0.01, 'validation_loss': 1.0555639266967773}\n",
      "{'loss': 0.99447661638259888, 'epoch': 23, 'learning_rate': 0.01, 'validation_loss': 0.93373316526412964}\n",
      "{'loss': 0.87409019470214844, 'epoch': 24, 'learning_rate': 0.01, 'validation_loss': 0.81675159931182861}\n",
      "{'loss': 0.76314437389373779, 'epoch': 25, 'learning_rate': 0.01, 'validation_loss': 0.71437543630599976}\n",
      "{'loss': 0.6707497239112854, 'epoch': 26, 'learning_rate': 0.005, 'validation_loss': 0.65100300312042236}\n",
      "{'loss': 0.63227152824401855, 'epoch': 27, 'learning_rate': 0.005, 'validation_loss': 0.61445385217666626}\n",
      "{'loss': 0.59745305776596069, 'epoch': 28, 'learning_rate': 0.005, 'validation_loss': 0.58118200302124023}\n",
      "{'loss': 0.56556427478790283, 'epoch': 29, 'learning_rate': 0.005, 'validation_loss': 0.55053472518920898}\n",
      "{'loss': 0.53603935241699219, 'epoch': 30, 'learning_rate': 0.005, 'validation_loss': 0.52203238010406494}\n",
      "{'loss': 0.50847643613815308, 'epoch': 31, 'learning_rate': 0.005, 'validation_loss': 0.49534046649932861}\n",
      "{'loss': 0.48259958624839783, 'epoch': 32, 'learning_rate': 0.005, 'validation_loss': 0.47023233771324158}\n",
      "{'loss': 0.45822036266326904, 'epoch': 33, 'learning_rate': 0.005, 'validation_loss': 0.44654944539070129}\n",
      "{'loss': 0.43520659208297729, 'epoch': 34, 'learning_rate': 0.005, 'validation_loss': 0.42418071627616882}\n",
      "{'loss': 0.4134623110294342, 'epoch': 35, 'learning_rate': 0.005, 'validation_loss': 0.40304234623908997}\n",
      "{'loss': 0.39291343092918396, 'epoch': 36, 'learning_rate': 0.005, 'validation_loss': 0.38306742906570435}\n",
      "{'loss': 0.37349817156791687, 'epoch': 37, 'learning_rate': 0.005, 'validation_loss': 0.36419880390167236}\n",
      "{'loss': 0.35516315698623657, 'epoch': 38, 'learning_rate': 0.005, 'validation_loss': 0.34638530015945435}\n",
      "{'loss': 0.33785882592201233, 'epoch': 39, 'learning_rate': 0.005, 'validation_loss': 0.32957804203033447}\n",
      "{'loss': 0.32153701782226562, 'epoch': 40, 'learning_rate': 0.005, 'validation_loss': 0.31372985243797302}\n",
      "{'loss': 0.30615103244781494, 'epoch': 41, 'learning_rate': 0.005, 'validation_loss': 0.29879435896873474}\n",
      "{'loss': 0.29165446758270264, 'epoch': 42, 'learning_rate': 0.005, 'validation_loss': 0.2847253680229187}\n",
      "{'loss': 0.27800169587135315, 'epoch': 43, 'learning_rate': 0.005, 'validation_loss': 0.27147760987281799}\n",
      "{'loss': 0.26514753699302673, 'epoch': 44, 'learning_rate': 0.005, 'validation_loss': 0.25900605320930481}\n",
      "{'loss': 0.25304758548736572, 'epoch': 45, 'learning_rate': 0.005, 'validation_loss': 0.24726676940917969}\n",
      "{'loss': 0.24165840446949005, 'epoch': 46, 'learning_rate': 0.005, 'validation_loss': 0.23621703684329987}\n",
      "{'loss': 0.23093798756599426, 'epoch': 47, 'learning_rate': 0.005, 'validation_loss': 0.22581568360328674}\n",
      "{'loss': 0.22084558010101318, 'epoch': 48, 'learning_rate': 0.005, 'validation_loss': 0.21602277457714081}\n",
      "{'loss': 0.21134269237518311, 'epoch': 49, 'learning_rate': 0.005, 'validation_loss': 0.20680059492588043}\n",
      "{'loss': 0.20239199697971344, 'epoch': 50, 'learning_rate': 0.005, 'validation_loss': 0.1981123685836792}\n",
      "{'loss': 0.19395802915096283, 'epoch': 51, 'learning_rate': 0.0025, 'validation_loss': 0.19193375110626221}\n",
      "{'loss': 0.18993915617465973, 'epoch': 52, 'learning_rate': 0.0025, 'validation_loss': 0.18797330558300018}\n",
      "{'loss': 0.18603616952896118, 'epoch': 53, 'learning_rate': 0.0025, 'validation_loss': 0.18412682414054871}\n",
      "{'loss': 0.18224538862705231, 'epoch': 54, 'learning_rate': 0.0025, 'validation_loss': 0.18039065599441528}\n",
      "{'loss': 0.1785627007484436, 'epoch': 55, 'learning_rate': 0.0025, 'validation_loss': 0.17676100134849548}\n",
      "{'loss': 0.17498509585857391, 'epoch': 56, 'learning_rate': 0.0025, 'validation_loss': 0.17323431372642517}\n",
      "{'loss': 0.17150881886482239, 'epoch': 57, 'learning_rate': 0.0025, 'validation_loss': 0.16980750858783722}\n",
      "{'loss': 0.16813047230243683, 'epoch': 58, 'learning_rate': 0.0025, 'validation_loss': 0.1664770245552063}\n",
      "{'loss': 0.16484692692756653, 'epoch': 59, 'learning_rate': 0.0025, 'validation_loss': 0.16323968768119812}\n",
      "{'loss': 0.16165490448474884, 'epoch': 60, 'learning_rate': 0.0025, 'validation_loss': 0.16009238362312317}\n",
      "{'loss': 0.15855157375335693, 'epoch': 61, 'learning_rate': 0.0025, 'validation_loss': 0.15703208744525909}\n",
      "{'loss': 0.15553362667560577, 'epoch': 62, 'learning_rate': 0.0025, 'validation_loss': 0.15405601263046265}\n",
      "{'loss': 0.15259861946105957, 'epoch': 63, 'learning_rate': 0.0025, 'validation_loss': 0.15116135776042938}\n",
      "{'loss': 0.14974366128444672, 'epoch': 64, 'learning_rate': 0.0025, 'validation_loss': 0.14834530651569366}\n",
      "{'loss': 0.14696599543094635, 'epoch': 65, 'learning_rate': 0.0025, 'validation_loss': 0.14560545980930328}\n",
      "{'loss': 0.14426311850547791, 'epoch': 66, 'learning_rate': 0.0025, 'validation_loss': 0.14293916523456573}\n",
      "{'loss': 0.14163282513618469, 'epoch': 67, 'learning_rate': 0.0025, 'validation_loss': 0.14034409821033478}\n",
      "{'loss': 0.13907238841056824, 'epoch': 68, 'learning_rate': 0.0025, 'validation_loss': 0.13781771063804626}\n",
      "{'loss': 0.13657966256141663, 'epoch': 69, 'learning_rate': 0.0025, 'validation_loss': 0.13535793125629425}\n",
      "{'loss': 0.13415242731571198, 'epoch': 70, 'learning_rate': 0.0025, 'validation_loss': 0.13296268880367279}\n",
      "{'loss': 0.13178856670856476, 'epoch': 71, 'learning_rate': 0.0025, 'validation_loss': 0.13062986731529236}\n",
      "{'loss': 0.12948606908321381, 'epoch': 72, 'learning_rate': 0.0025, 'validation_loss': 0.12835714221000671}\n",
      "{'loss': 0.1272427886724472, 'epoch': 73, 'learning_rate': 0.0025, 'validation_loss': 0.12614290416240692}\n",
      "{'loss': 0.1250569224357605, 'epoch': 74, 'learning_rate': 0.0025, 'validation_loss': 0.12398497760295868}\n",
      "{'loss': 0.12292660027742386, 'epoch': 75, 'learning_rate': 0.0025, 'validation_loss': 0.12188185006380081}\n",
      "{'loss': 0.1208501011133194, 'epoch': 76, 'learning_rate': 0.00125, 'validation_loss': 0.12034015357494354}\n",
      "{'loss': 0.11983317136764526, 'epoch': 77, 'learning_rate': 0.00125, 'validation_loss': 0.11932939291000366}\n",
      "{'loss': 0.11882892251014709, 'epoch': 78, 'learning_rate': 0.00125, 'validation_loss': 0.11833161860704422}\n",
      "{'loss': 0.11783723533153534, 'epoch': 79, 'learning_rate': 0.00125, 'validation_loss': 0.1173461377620697}\n",
      "{'loss': 0.11685783416032791, 'epoch': 80, 'learning_rate': 0.00125, 'validation_loss': 0.1163727343082428}\n",
      "{'loss': 0.11589085310697556, 'epoch': 81, 'learning_rate': 0.00125, 'validation_loss': 0.11541172116994858}\n",
      "{'loss': 0.11493557691574097, 'epoch': 82, 'learning_rate': 0.00125, 'validation_loss': 0.1144624650478363}\n",
      "{'loss': 0.11399224400520325, 'epoch': 83, 'learning_rate': 0.00125, 'validation_loss': 0.11352488398551941}\n",
      "{'loss': 0.11306025832891464, 'epoch': 84, 'learning_rate': 0.00125, 'validation_loss': 0.11259861290454865}\n",
      "{'loss': 0.11213985085487366, 'epoch': 85, 'learning_rate': 0.00125, 'validation_loss': 0.1116839200258255}\n",
      "{'loss': 0.11123071610927582, 'epoch': 86, 'learning_rate': 0.00125, 'validation_loss': 0.11078041046857834}\n",
      "{'loss': 0.11033274233341217, 'epoch': 87, 'learning_rate': 0.00125, 'validation_loss': 0.10988755524158478}\n",
      "{'loss': 0.10944537818431854, 'epoch': 88, 'learning_rate': 0.00125, 'validation_loss': 0.10900592058897018}\n",
      "{'loss': 0.10856883227825165, 'epoch': 89, 'learning_rate': 0.00125, 'validation_loss': 0.10813458263874054}\n",
      "{'loss': 0.10770286619663239, 'epoch': 90, 'learning_rate': 0.00125, 'validation_loss': 0.1072738841176033}\n",
      "{'loss': 0.10684731602668762, 'epoch': 91, 'learning_rate': 0.00125, 'validation_loss': 0.10642347484827042}\n",
      "{'loss': 0.10600193589925766, 'epoch': 92, 'learning_rate': 0.00125, 'validation_loss': 0.10558322817087173}\n",
      "{'loss': 0.10516670346260071, 'epoch': 93, 'learning_rate': 0.00125, 'validation_loss': 0.10475284606218338}\n",
      "{'loss': 0.10434141755104065, 'epoch': 94, 'learning_rate': 0.00125, 'validation_loss': 0.10393235087394714}\n",
      "{'loss': 0.10352572053670883, 'epoch': 95, 'learning_rate': 0.00125, 'validation_loss': 0.10312159359455109}\n",
      "{'loss': 0.10271978378295898, 'epoch': 96, 'learning_rate': 0.00125, 'validation_loss': 0.10232042521238327}\n",
      "{'loss': 0.10192317515611649, 'epoch': 97, 'learning_rate': 0.00125, 'validation_loss': 0.10152851790189743}\n",
      "{'loss': 0.10113618522882462, 'epoch': 98, 'learning_rate': 0.00125, 'validation_loss': 0.10074596852064133}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.10035804659128189, 'epoch': 99, 'learning_rate': 0.00125, 'validation_loss': 0.099972523748874664}\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/MemN2N.model-182\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = MemN2N(FLAGS, sess)\n",
    "    model.build_model()\n",
    "\n",
    "    if FLAGS.is_test:\n",
    "        model.run(valid_stories, valid_questions, test_stories, test_questions)\n",
    "    else:\n",
    "        model.run(train_stories, train_questions, valid_stories, valid_questions)\n",
    "        \n",
    "    predictions, target = model.predict(train_stories, train_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story words:\n",
      "[['나', '는', '지금', '배가', '고프다'],\n",
      " ['나', '는', '집에', '있다가', '왔다'],\n",
      " ['나', '는', '지금', '판교에', '있다']]\n",
      "\n",
      "Question:\n",
      "['너', '는', '지금', '어디', '있니']\n",
      "\n",
      "Prediction:\n",
      "['판교']\n",
      "\n",
      "Answer:\n",
      "['판교']\n",
      "\n",
      "Correct:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = 0\n",
    "\n",
    "depad_data(train_stories, train_questions)\n",
    "\n",
    "question = train_questions[index]['question']\n",
    "answer = train_questions[index]['answer']\n",
    "story_index = train_questions[index]['story_index']\n",
    "sentence_index = train_questions[index]['sentence_index']\n",
    "\n",
    "story = train_stories[story_index][:sentence_index + 1]\n",
    "\n",
    "story = [list(map(idx2word.get, sentence)) for sentence in story]\n",
    "question = list(map(idx2word.get, question))\n",
    "prediction = [idx2word[np.argmax(predictions[index])]]\n",
    "answer = list(map(idx2word.get, answer))\n",
    "\n",
    "print('Story words:')\n",
    "pp.pprint(story)\n",
    "print('\\nQuestion:')\n",
    "pp.pprint(question)\n",
    "print('\\nPrediction:')\n",
    "pp.pprint(prediction)\n",
    "print('\\nAnswer:')\n",
    "pp.pprint(answer)\n",
    "print('\\nCorrect:')\n",
    "pp.pprint(prediction == answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "['너', '는', '무엇', '을', '주문', '할려구']\n",
      "\n",
      "Prediction:\n",
      "['피자']\n",
      "\n",
      "Answer:\n",
      "['피자']\n",
      "\n",
      "Correct:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "\n",
    "depad_data(train_stories, train_questions)\n",
    "\n",
    "question = train_questions[index]['question']\n",
    "answer = train_questions[index]['answer']\n",
    "story_index = train_questions[index]['story_index']\n",
    "sentence_index = train_questions[index]['sentence_index']\n",
    "\n",
    "story = train_stories[story_index][:sentence_index + 1]\n",
    "\n",
    "story = [list(map(idx2word.get, sentence)) for sentence in story]\n",
    "question = list(map(idx2word.get, question))\n",
    "prediction = [idx2word[np.argmax(predictions[index])]]\n",
    "answer = list(map(idx2word.get, answer))\n",
    "\n",
    "print('\\nQuestion:')\n",
    "pp.pprint(question)\n",
    "print('\\nPrediction:')\n",
    "pp.pprint(prediction)\n",
    "print('\\nAnswer:')\n",
    "pp.pprint(answer)\n",
    "print('\\nCorrect:')\n",
    "pp.pprint(prediction == answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
