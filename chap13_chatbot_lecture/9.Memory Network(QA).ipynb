{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 13:51:32) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print (sys.version)\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines= ['1 John travelled to the hallway.',\n",
    "'2 김수상 journeyed to the bathroom.',\n",
    "'3 Where is John? \thallway\t1',\n",
    "'4 Daniel went back to the bathroom.',\n",
    "'5 John moved to the bedroom.',\n",
    "'6 Where is 김수상? \tbathroom\t2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력값의 명사를 통해 완전한 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def read_data(fname, word2idx, max_words, max_sentences):\n",
    "    # stories[story_ind] = [[sentence1], [sentence2], ..., [sentenceN]]\n",
    "    # questions[question_ind] = {'question': [question], 'answer': [answer], 'story_index': #, 'sentence_index': #}\n",
    "    stories = dict()\n",
    "    questions = dict()\n",
    "    \n",
    "    \n",
    "    if len(word2idx) == 0:\n",
    "        word2idx['<null>'] = 0\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        max_words = max(max_words, len(words))\n",
    "        \n",
    "        # Determine whether the line indicates the start of a new story\n",
    "        if words[0] == '1':\n",
    "            story_ind = len(stories)\n",
    "            sentence_ind = 0\n",
    "            stories[story_ind] = []\n",
    "        \n",
    "        # Determine whether the line is a question or not\n",
    "        if '?' in line:\n",
    "            is_question = True\n",
    "            question_ind = len(questions)\n",
    "            questions[question_ind] = {'question': [], 'answer': [], 'story_index': story_ind, 'sentence_index': sentence_ind}\n",
    "        else:\n",
    "            is_question = False\n",
    "            sentence_ind = len(stories[story_ind])\n",
    "        \n",
    "        # Parse and append the words to appropriate dictionary / Expand word2idx dictionary\n",
    "        sentence_list = []\n",
    "        for k in range(1, len(words)):\n",
    "            w = words[k].lower()\n",
    "            \n",
    "            # Remove punctuation\n",
    "            if ('.' in w) or ('?' in w):\n",
    "                w = w[:-1]\n",
    "            \n",
    "            # Add new word to dictionary\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = len(word2idx)\n",
    "            \n",
    "            # Append sentence to story dict if not question\n",
    "            if not is_question:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '.' in words[k]:\n",
    "                    stories[story_ind].append(sentence_list)\n",
    "                    break\n",
    "            \n",
    "            # Append sentence and answer to question dict if question\n",
    "            else:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '?' in words[k]:\n",
    "                    answer = words[k + 1].lower()\n",
    "                    \n",
    "                    if answer not in word2idx:\n",
    "                        word2idx[answer] = len(word2idx)\n",
    "                    \n",
    "                    questions[question_ind]['question'].extend(sentence_list)\n",
    "                    questions[question_ind]['answer'].append(answer)\n",
    "                    break\n",
    "        \n",
    "        # Update max_sentences\n",
    "        max_sentences = max(max_sentences, sentence_ind+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert the words into indices\n",
    "    for idx, context in stories.items():\n",
    "        for i in range(len(context)):\n",
    "            temp = list(map(word2idx.get, context[i]))\n",
    "            context[i] = temp\n",
    "    \n",
    "    for idx, value in questions.items():\n",
    "        temp1 = list(map(word2idx.get, value['question']))\n",
    "        temp2 = list(map(word2idx.get, value['answer']))\n",
    "        \n",
    "        value['question'] = temp1\n",
    "        value['answer'] = temp2\n",
    "    \n",
    "    return stories, questions, max_words, max_sentences\n",
    "\n",
    "\n",
    "def pad_data(stories, questions, max_words, max_sentences):\n",
    "\n",
    "    # Pad the context into same size with '<null>'\n",
    "    for idx, context in stories.items():\n",
    "        for sentence in context:           \n",
    "            while len(sentence) < max_words:\n",
    "                sentence.append(0)\n",
    "        while len(context) < max_sentences:\n",
    "            context.append([0] * max_words)\n",
    "    \n",
    "    # Pad the question into same size with '<null>'\n",
    "    for idx, value in questions.items():\n",
    "        while len(value['question']) < max_words:\n",
    "            value['question'].append(0)\n",
    "\n",
    "\n",
    "def depad_data(stories, questions):\n",
    "\n",
    "    for idx, context in stories.items():\n",
    "        for i in range(len(context)):\n",
    "            if 0 in context[i]:\n",
    "                if context[i][0] == 0:\n",
    "                    temp = context[:i]\n",
    "                    context = temp\n",
    "                    break\n",
    "                else:\n",
    "                    index = context[i].index(0)\n",
    "                    context[i] = context[i][:index]\n",
    "\n",
    "    for idx, value in questions.items():\n",
    "        if 0 in value['question']:\n",
    "            index = value['question'].index(0)\n",
    "            value['question'] = value['question'][:index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anneal_epoch': 25,\n",
      " 'anneal_rate': 0.5,\n",
      " 'babi_task': 1,\n",
      " 'batch_size': 32,\n",
      " 'checkpoint_dir': './checkpoints',\n",
      " 'data_dir': './bAbI/en-valid',\n",
      " 'edim': 20,\n",
      " 'init_lr': 0.01,\n",
      " 'init_mean': 0.0,\n",
      " 'init_std': 0.1,\n",
      " 'is_test': False,\n",
      " 'lin_start': False,\n",
      " 'max_grad_norm': 40,\n",
      " 'max_sentences': 4,\n",
      " 'max_words': 7,\n",
      " 'mem_size': 50,\n",
      " 'nepoch': 100,\n",
      " 'nhop': 3,\n",
      " 'nwords': 16,\n",
      " 'show_progress': False}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_integer(\"edim\", 20, \"internal state dimension [20]\")\n",
    "flags.DEFINE_integer(\"nhop\", 3, \"number of hops [3]\")\n",
    "flags.DEFINE_integer(\"mem_size\", 50, \"maximum number of sentences that can be encoded into memory [50]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"batch size to use during training [32]\")\n",
    "flags.DEFINE_integer(\"nepoch\", 100, \"number of epoch to use during training [100]\")\n",
    "flags.DEFINE_integer(\"anneal_epoch\", 25, \"anneal the learning rate every <anneal_epoch> epochs [25]\")\n",
    "flags.DEFINE_integer(\"babi_task\", 1, \"index of bAbI task for the network to learn [1]\")\n",
    "flags.DEFINE_float(\"init_lr\", 0.01, \"initial learning rate [0.01]\")\n",
    "flags.DEFINE_float(\"anneal_rate\", 0.5, \"learning rate annealing rate [0.5]\")\n",
    "flags.DEFINE_float(\"init_mean\", 0., \"weight initialization mean [0.]\")\n",
    "flags.DEFINE_float(\"init_std\", 0.1, \"weight initialization std [0.1]\")\n",
    "flags.DEFINE_float(\"max_grad_norm\", 40, \"clip gradients to this norm [40]\")\n",
    "flags.DEFINE_string(\"data_dir\", \"./bAbI/en-valid\", \"dataset directory [./bAbI/en_valid]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"./checkpoints\", \"checkpoint directory [./checkpoints]\")\n",
    "flags.DEFINE_boolean(\"lin_start\", False, \"True for linear start training, False for otherwise [False]\")\n",
    "flags.DEFINE_boolean(\"is_test\", False, \"True for testing, False for training [False]\")\n",
    "flags.DEFINE_boolean(\"show_progress\", False, \"print progress [False]\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "word2idx = {}\n",
    "max_words = 0\n",
    "max_sentences = 0\n",
    "\n",
    "train_stories, train_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "valid_stories, valid_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "test_stories, test_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "\n",
    "pad_data(train_stories, train_questions, max_words, max_sentences)\n",
    "pad_data(valid_stories, valid_questions, max_words, max_sentences)\n",
    "pad_data(test_stories, test_questions, max_words, max_sentences)\n",
    "\n",
    "idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "FLAGS.nwords = len(word2idx)\n",
    "FLAGS.max_words = max_words\n",
    "FLAGS.max_sentences = max_sentences\n",
    "\n",
    "pp.pprint(flags.FLAGS.__flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ProgressBar(Bar):\n",
    "    message = 'Loading'\n",
    "    fill = '#'\n",
    "    suffix = '%(percent).1f%% | ETA: %(eta)ds'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습결과 출력\n",
    "* Memory Network 학습 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class MemN2N(object):\n",
    "    \n",
    "    def __init__(self, config, sess):\n",
    "        self.nwords = config.nwords\n",
    "        self.max_words = config.max_words\n",
    "        self.max_sentences = config.max_sentences\n",
    "        self.init_mean = config.init_mean\n",
    "        self.init_std = config.init_std\n",
    "        self.batch_size = config.batch_size\n",
    "        self.nepoch = config.nepoch\n",
    "        self.anneal_epoch = config.anneal_epoch\n",
    "        self.nhop = config.nhop\n",
    "        self.edim = config.edim\n",
    "        self.mem_size = config.mem_size\n",
    "        self.max_grad_norm = config.max_grad_norm\n",
    "        \n",
    "        self.lin_start = config.lin_start\n",
    "        self.show_progress = config.show_progress\n",
    "        self.is_test = config.is_test\n",
    "\n",
    "        self.checkpoint_dir = config.checkpoint_dir\n",
    "        \n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        \n",
    "        self.query = tf.placeholder(tf.int32, [None, self.max_words], name='input')\n",
    "        self.time = tf.placeholder(tf.int32, [None, self.mem_size], name='time')\n",
    "        self.target = tf.placeholder(tf.float32, [None, self.nwords], name='target')\n",
    "        self.context = tf.placeholder(tf.int32, [None, self.mem_size, self.max_words], name='context')\n",
    "        \n",
    "        self.hid = []\n",
    "        \n",
    "        self.lr = None\n",
    "        \n",
    "        if self.lin_start:\n",
    "            self.current_lr = 0.005\n",
    "        else:\n",
    "            self.current_lr = config.init_lr\n",
    "\n",
    "        self.anneal_rate = config.anneal_rate\n",
    "        self.loss = None\n",
    "        self.optim = None\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.log_loss = []\n",
    "        self.log_perp = []\n",
    "    \n",
    "    def build_memory(self):\n",
    "        self.global_step = tf.Variable(0, name='global_step')\n",
    "        \n",
    "        zeros = tf.constant(0, tf.float32, [1, self.edim])\n",
    "        self.A_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.B_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.C_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        \n",
    "        A = tf.concat([zeros, self.A_], axis=0)\n",
    "        B = tf.concat([zeros, self.B_], axis=0)\n",
    "        C = tf.concat([zeros, self.C_], axis=0)\n",
    "        \n",
    "        self.T_A_ = tf.Variable(tf.random_normal([self.mem_size - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.T_C_ = tf.Variable(tf.random_normal([self.mem_size - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        \n",
    "        T_A = tf.concat([zeros, self.T_A_], axis=0)\n",
    "        T_C = tf.concat([zeros, self.T_C_], axis=0)\n",
    "        \n",
    "        A_ebd = tf.nn.embedding_lookup(A, self.context)   # [batch_size, mem_size, max_length, edim]\n",
    "        A_ebd = tf.reduce_sum(A_ebd, axis=2)              # [batch_size, mem_size, edim]\n",
    "        T_A_ebd = tf.nn.embedding_lookup(T_A, self.time)  # [batch_size, mem_size, edim]\n",
    "        A_in = tf.add(A_ebd, T_A_ebd)                     # [batch_size, mem_size, edim]\n",
    "        \n",
    "        C_ebd = tf.nn.embedding_lookup(C, self.context)   # [batch_size, mem_size, max_length, edim]\n",
    "        C_ebd = tf.reduce_sum(C_ebd, axis=2)              # [batch_size, mem_size, edim]\n",
    "        T_C_ebd = tf.nn.embedding_lookup(T_C, self.time)  # [batch_size, mem_size, edim]\n",
    "        C_in = tf.add(C_ebd, T_C_ebd)                     # [batch_size, mem_size, edim]\n",
    "        \n",
    "        query_ebd = tf.nn.embedding_lookup(B, self.query) # [batch_size, max_length, edim]\n",
    "        query_ebd = tf.reduce_sum(query_ebd, axis=1)      # [batch_size, edim]\n",
    "        self.hid.append(query_ebd)\n",
    "        \n",
    "        for h in range(self.nhop):\n",
    "            q3dim = tf.reshape(self.hid[-1], [-1, 1, self.edim]) # [batch_size, edim] ==> [batch_size, 1, edim]\n",
    "            p3dim = tf.matmul(q3dim, A_in, transpose_b=True)     # [batch_size, 1, edim] X [batch_size, edim, mem_size]\n",
    "            p2dim = tf.reshape(p3dim, [-1, self.mem_size])       # [batch_size, mem_size]\n",
    "            \n",
    "            # If linear start, remove softmax layers\n",
    "            if self.lin_start:\n",
    "                p = p2dim\n",
    "            else:\n",
    "                p = tf.nn.softmax(p2dim)\n",
    "            \n",
    "            p3dim = tf.reshape(p, [-1, 1, self.mem_size]) # [batch_size, 1, mem_size]\n",
    "            o3dim = tf.matmul(p3dim, C_in)                # [batch_size, 1, mem_size] X [batch_size, mem_size, edim]\n",
    "            o2dim = tf.reshape(o3dim, [-1, self.edim])    # [batch_size, edim]\n",
    "            \n",
    "            a = tf.add(o2dim, self.hid[-1]) # [batch_size, edim]\n",
    "            self.hid.append(a)              # [input, a_1, a_2, ..., a_nhop]\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.build_memory()\n",
    "        \n",
    "        self.W = tf.Variable(tf.random_normal([self.edim, self.nwords], mean=self.init_mean, stddev=self.init_std))\n",
    "        a_hat = tf.matmul(self.hid[-1], self.W)\n",
    "        \n",
    "        self.hypothesis = tf.nn.softmax(a_hat)\n",
    "\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=a_hat, labels=self.target)\n",
    "        \n",
    "        self.lr = tf.Variable(self.current_lr)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        \n",
    "        params = [self.A_, self.B_, self.C_, self.T_A_, self.T_C_, self.W]\n",
    "        grads_and_vars = self.opt.compute_gradients(self.loss, params)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], self.max_grad_norm), gv[1]) for gv in grads_and_vars]\n",
    "        \n",
    "        inc = self.global_step.assign_add(1)\n",
    "        with tf.control_dependencies([inc]):\n",
    "            self.optim = self.opt.apply_gradients(clipped_grads_and_vars)\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    def train(self, train_stories, train_questions):\n",
    "        N = int(math.ceil(len(train_questions) / self.batch_size))\n",
    "        cost = 0\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "        \n",
    "        for idx in range(N):\n",
    "            \n",
    "            if self.show_progress:\n",
    "                bar.next()\n",
    "            \n",
    "            if idx == N - 1:\n",
    "                iterations = len(train_questions) - (N - 1) * self.batch_size\n",
    "            else:\n",
    "                iterations = self.batch_size\n",
    "            \n",
    "            query = np.ndarray([iterations, self.max_words], dtype=np.int32)\n",
    "            time = np.zeros([iterations, self.mem_size], dtype=np.int32)\n",
    "            target = np.zeros([iterations, self.nwords], dtype=np.float32)\n",
    "            context = np.ndarray([iterations, self.mem_size, self.max_words], dtype=np.int32)\n",
    "            \n",
    "            for b in range(iterations):\n",
    "                m = idx * self.batch_size + b\n",
    "                \n",
    "                curr_q = train_questions[m]\n",
    "                q_text = curr_q['question']\n",
    "                story_ind = curr_q['story_index']\n",
    "                sent_ind = curr_q['sentence_index']\n",
    "                answer = curr_q['answer'][0]\n",
    "                \n",
    "                curr_s = train_stories[story_ind]\n",
    "                curr_c = curr_s[:sent_ind + 1]\n",
    "\n",
    "                if len(curr_c) >= self.mem_size:\n",
    "                    curr_c = curr_c[-self.mem_size:]\n",
    "                    \n",
    "                    for t in range(self.mem_size):\n",
    "                        time[b, t].fill(t)\n",
    "                else:\n",
    "                    \n",
    "                    for t in range(len(curr_c)):\n",
    "                        time[b, t].fill(t)\n",
    "                    \n",
    "                    while len(curr_c) < self.mem_size:\n",
    "                        curr_c.append([0.] * self.max_words)\n",
    "                \n",
    "                query[b, :] = q_text\n",
    "                target[b, answer] = 1\n",
    "                context[b, :, :] = curr_c\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim, self.loss, self.global_step],\n",
    "                                               feed_dict={self.query: query, self.time: time,\n",
    "                                                          self.target: target, self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar.finish()\n",
    "        \n",
    "        return cost / len(train_questions)\n",
    "    \n",
    "    \n",
    "    def test(self, test_stories, test_questions, label='Test'):\n",
    "        N = int(math.ceil(len(test_questions) / self.batch_size))\n",
    "        cost = 0\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "        \n",
    "        for idx in range(N):\n",
    "            \n",
    "            if self.show_progress:\n",
    "                bar.next()\n",
    "            \n",
    "            if idx == N - 1:\n",
    "                iterations = len(test_questions) - (N - 1) * self.batch_size\n",
    "            else:\n",
    "                iterations = self.batch_size\n",
    "            \n",
    "            query = np.ndarray([iterations, self.max_words], dtype=np.int32)\n",
    "            time = np.zeros([iterations, self.mem_size], dtype=np.int32)\n",
    "            target = np.zeros([iterations, self.nwords], dtype=np.float32)\n",
    "            context = np.ndarray([iterations, self.mem_size, self.max_words], dtype=np.int32)\n",
    "            \n",
    "            for b in range(iterations):\n",
    "                m = idx * self.batch_size + b\n",
    "                \n",
    "                curr_q = test_questions[m]\n",
    "                q_text = curr_q['question']\n",
    "                story_ind = curr_q['story_index']\n",
    "                sent_ind = curr_q['sentence_index']\n",
    "                answer = curr_q['answer'][0]\n",
    "                \n",
    "                curr_s = test_stories[story_ind]\n",
    "                curr_c = curr_s[:sent_ind + 1]\n",
    "                \n",
    "                if len(curr_c) >= self.mem_size:\n",
    "                    curr_c = curr_c[-self.mem_size:]\n",
    "                    \n",
    "                    for t in range(self.mem_size):\n",
    "                        time[b, t].fill(t)\n",
    "                else:\n",
    "                    \n",
    "                    for t in range(len(curr_c)):\n",
    "                        time[b, t].fill(t)\n",
    "                    \n",
    "                    while len(curr_c) < self.mem_size:\n",
    "                        curr_c.append([0.] * self.max_words)\n",
    "                \n",
    "                query[b, :] = q_text\n",
    "                target[b, answer] = 1\n",
    "                context[b, :, :] = curr_c\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim, self.loss, self.global_step],\n",
    "                                               feed_dict={self.query: query, self.time: time,\n",
    "                                                          self.target: target, self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar.finish()\n",
    "        \n",
    "        return cost / len(test_questions)\n",
    "    \n",
    "    \n",
    "    def run(self, train_stories, train_questions, test_stories, test_questions):\n",
    "        if not self.is_test:# add not\n",
    "\n",
    "            for idx in range(self.nepoch):\n",
    "                train_loss = np.sum(self.train(train_stories, train_questions))\n",
    "                test_loss = np.sum(self.test(test_stories, test_questions, label='Validation'))\n",
    "                \n",
    "                self.log_loss.append([train_loss, test_loss])\n",
    "                \n",
    "                state = {\n",
    "                    'loss': train_loss,\n",
    "                    'epoch': idx,\n",
    "                    'learning_rate': self.current_lr,\n",
    "                    'validation_loss': test_loss\n",
    "                }\n",
    "                \n",
    "                print(state)\n",
    "                \n",
    "                \n",
    "                # learning rate annealing\n",
    "                if (not idx == 0) and (idx % self.anneal_epoch == 0):\n",
    "                    self.current_lr = self.current_lr * self.anneal_rate\n",
    "                    self.lr.assign(self.current_lr).eval()\n",
    "            \n",
    "                # If validation loss stops decreasing, insert softmax layers\n",
    "                if idx == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    if self.log_loss[idx][1] > self.log_loss[idx - 1][1]:\n",
    "                        self.lin_start = False\n",
    "\n",
    "                if idx % 10 == 0:\n",
    "                    self.saver.save(self.sess,\n",
    "                                    os.path.join(self.checkpoint_dir, \"MemN2N.model\"),\n",
    "                                    global_step=self.step.astype(int))\n",
    "        else:\n",
    "            self.load()\n",
    "            \n",
    "            valid_loss = np.sum(self.test(train_stories, train_questions, label='Validation'))\n",
    "            test_loss = np.sum(self.test(test_stories, test_questions, label='Test'))\n",
    "            \n",
    "            state = {\n",
    "                'validation_loss': valid_loss,\n",
    "                'test_loss': test_loss\n",
    "            }\n",
    "            \n",
    "            print(state)\n",
    "\n",
    "\n",
    "    def predict(self, test_stories, test_questions):\n",
    "        self.load()\n",
    "\n",
    "        num_instances = len(test_questions)\n",
    "\n",
    "        query = np.ndarray([num_instances, self.max_words], dtype=np.int32)\n",
    "        time = np.zeros([num_instances, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([num_instances, self.nwords], dtype=np.float32)\n",
    "        context = np.ndarray([num_instances, self.mem_size, self.max_words], dtype=np.int32)\n",
    "\n",
    "        for b in range(num_instances):\n",
    "            \n",
    "            curr_q = test_questions[b]\n",
    "            q_text = curr_q['question']\n",
    "            story_ind = curr_q['story_index']\n",
    "            sent_ind = curr_q['sentence_index']\n",
    "            answer = curr_q['answer'][0]\n",
    "            \n",
    "            curr_s = test_stories[story_ind]\n",
    "            curr_c = curr_s[:sent_ind + 1]\n",
    "            \n",
    "            if len(curr_c) >= self.mem_size:\n",
    "                curr_c = curr_c[-self.mem_size:]\n",
    "                \n",
    "                for t in range(self.mem_size):\n",
    "                    time[b, t].fill(t)\n",
    "            else:\n",
    "                \n",
    "                for t in range(len(curr_c)):\n",
    "                    time[b, t].fill(t)\n",
    "                \n",
    "                while len(curr_c) < self.mem_size:\n",
    "                    curr_c.append([0.] * self.max_words)\n",
    "            \n",
    "            query[b, :] = q_text\n",
    "            target[b, answer] = 1\n",
    "            context[b, :, :] = curr_c\n",
    "\n",
    "        predictions = self.sess.run(self.hypothesis, feed_dict={self.query: query, self.time: time, self.context: context})\n",
    "\n",
    "        return predictions, target\n",
    "\n",
    "\n",
    "        \n",
    "    def load(self):\n",
    "        print(' [*] Reading checkpoints...')\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\" [!] No checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6178379058837891, 'epoch': 0, 'learning_rate': 0.01, 'validation_loss': 2.6003973484039307}\n",
      "{'loss': 2.5827445983886719, 'epoch': 1, 'learning_rate': 0.01, 'validation_loss': 2.5648550987243652}\n",
      "{'loss': 2.5467033386230469, 'epoch': 2, 'learning_rate': 0.01, 'validation_loss': 2.5282645225524902}\n",
      "{'loss': 2.5095133781433105, 'epoch': 3, 'learning_rate': 0.01, 'validation_loss': 2.4904255867004395}\n",
      "{'loss': 2.4709763526916504, 'epoch': 4, 'learning_rate': 0.01, 'validation_loss': 2.4511404037475586}\n",
      "{'loss': 2.4308938980102539, 'epoch': 5, 'learning_rate': 0.01, 'validation_loss': 2.4102118015289307}\n",
      "{'loss': 2.3890700340270996, 'epoch': 6, 'learning_rate': 0.01, 'validation_loss': 2.3674442768096924}\n",
      "{'loss': 2.3453102111816406, 'epoch': 7, 'learning_rate': 0.01, 'validation_loss': 2.322643518447876}\n",
      "{'loss': 2.2994208335876465, 'epoch': 8, 'learning_rate': 0.01, 'validation_loss': 2.2756175994873047}\n",
      "{'loss': 2.2512111663818359, 'epoch': 9, 'learning_rate': 0.01, 'validation_loss': 2.2261772155761719}\n",
      "{'loss': 2.2004923820495605, 'epoch': 10, 'learning_rate': 0.01, 'validation_loss': 2.1741342544555664}\n",
      "{'loss': 2.1470789909362793, 'epoch': 11, 'learning_rate': 0.01, 'validation_loss': 2.1193041801452637}\n",
      "{'loss': 2.0907869338989258, 'epoch': 12, 'learning_rate': 0.01, 'validation_loss': 2.0615043640136719}\n",
      "{'loss': 2.03143310546875, 'epoch': 13, 'learning_rate': 0.01, 'validation_loss': 2.0005497932434082}\n",
      "{'loss': 1.9688305854797363, 'epoch': 14, 'learning_rate': 0.01, 'validation_loss': 1.9362504482269287}\n",
      "{'loss': 1.9027831554412842, 'epoch': 15, 'learning_rate': 0.01, 'validation_loss': 1.8684008121490479}\n",
      "{'loss': 1.8330725431442261, 'epoch': 16, 'learning_rate': 0.01, 'validation_loss': 1.7967644929885864}\n",
      "{'loss': 1.7594377994537354, 'epoch': 17, 'learning_rate': 0.01, 'validation_loss': 1.7210485935211182}\n",
      "{'loss': 1.6815445423126221, 'epoch': 18, 'learning_rate': 0.01, 'validation_loss': 1.640864372253418}\n",
      "{'loss': 1.5989347696304321, 'epoch': 19, 'learning_rate': 0.01, 'validation_loss': 1.5556681156158447}\n",
      "{'loss': 1.5109593868255615, 'epoch': 20, 'learning_rate': 0.01, 'validation_loss': 1.4646838903427124}\n",
      "{'loss': 1.4166954755783081, 'epoch': 21, 'learning_rate': 0.01, 'validation_loss': 1.3668287992477417}\n",
      "{'loss': 1.3149051666259766, 'epoch': 22, 'learning_rate': 0.01, 'validation_loss': 1.260751485824585}\n",
      "{'loss': 1.2042394876480103, 'epoch': 23, 'learning_rate': 0.01, 'validation_loss': 1.1453561782836914}\n",
      "{'loss': 1.0843230485916138, 'epoch': 24, 'learning_rate': 0.01, 'validation_loss': 1.0217609405517578}\n",
      "{'loss': 0.95885628461837769, 'epoch': 25, 'learning_rate': 0.01, 'validation_loss': 0.89739036560058594}\n",
      "{'loss': 0.83945953845977783, 'epoch': 26, 'learning_rate': 0.005, 'validation_loss': 0.81275278329849243}\n",
      "{'loss': 0.78752970695495605, 'epoch': 27, 'learning_rate': 0.005, 'validation_loss': 0.76384514570236206}\n",
      "{'loss': 0.74169057607650757, 'epoch': 28, 'learning_rate': 0.005, 'validation_loss': 0.72100555896759033}\n",
      "{'loss': 0.70169240236282349, 'epoch': 29, 'learning_rate': 0.005, 'validation_loss': 0.68363189697265625}\n",
      "{'loss': 0.66669631004333496, 'epoch': 30, 'learning_rate': 0.005, 'validation_loss': 0.65075886249542236}\n",
      "{'loss': 0.63570022583007812, 'epoch': 31, 'learning_rate': 0.005, 'validation_loss': 0.62141269445419312}\n",
      "{'loss': 0.60780096054077148, 'epoch': 32, 'learning_rate': 0.005, 'validation_loss': 0.59478247165679932}\n",
      "{'loss': 0.58228731155395508, 'epoch': 33, 'learning_rate': 0.005, 'validation_loss': 0.57025551795959473}\n",
      "{'loss': 0.55863738059997559, 'epoch': 34, 'learning_rate': 0.005, 'validation_loss': 0.54739058017730713}\n",
      "{'loss': 0.53647953271865845, 'epoch': 35, 'learning_rate': 0.005, 'validation_loss': 0.52587461471557617}\n",
      "{'loss': 0.51555007696151733, 'epoch': 36, 'learning_rate': 0.005, 'validation_loss': 0.50548428297042847}\n",
      "{'loss': 0.49565809965133667, 'epoch': 37, 'learning_rate': 0.005, 'validation_loss': 0.48605507612228394}\n",
      "{'loss': 0.47666081786155701, 'epoch': 38, 'learning_rate': 0.005, 'validation_loss': 0.46746250987052917}\n",
      "{'loss': 0.45844835042953491, 'epoch': 39, 'learning_rate': 0.005, 'validation_loss': 0.44960814714431763}\n",
      "{'loss': 0.44093248248100281, 'epoch': 40, 'learning_rate': 0.005, 'validation_loss': 0.43241313099861145}\n",
      "{'loss': 0.42404252290725708, 'epoch': 41, 'learning_rate': 0.005, 'validation_loss': 0.4158138632774353}\n",
      "{'loss': 0.40772107243537903, 'epoch': 42, 'learning_rate': 0.005, 'validation_loss': 0.39975875616073608}\n",
      "{'loss': 0.39192250370979309, 'epoch': 43, 'learning_rate': 0.005, 'validation_loss': 0.38420772552490234}\n",
      "{'loss': 0.37661117315292358, 'epoch': 44, 'learning_rate': 0.005, 'validation_loss': 0.36912977695465088}\n",
      "{'loss': 0.36176076531410217, 'epoch': 45, 'learning_rate': 0.005, 'validation_loss': 0.35450196266174316}\n",
      "{'loss': 0.34735175967216492, 'epoch': 46, 'learning_rate': 0.005, 'validation_loss': 0.34030908346176147}\n",
      "{'loss': 0.33337253332138062, 'epoch': 47, 'learning_rate': 0.005, 'validation_loss': 0.3265412449836731}\n",
      "{'loss': 0.31981492042541504, 'epoch': 48, 'learning_rate': 0.005, 'validation_loss': 0.31319314241409302}\n",
      "{'loss': 0.30667597055435181, 'epoch': 49, 'learning_rate': 0.005, 'validation_loss': 0.30026283860206604}\n",
      "{'loss': 0.29395449161529541, 'epoch': 50, 'learning_rate': 0.005, 'validation_loss': 0.28775081038475037}\n",
      "{'loss': 0.28165197372436523, 'epoch': 51, 'learning_rate': 0.0025, 'validation_loss': 0.27864843606948853}\n",
      "{'loss': 0.27567103505134583, 'epoch': 52, 'learning_rate': 0.0025, 'validation_loss': 0.27271997928619385}\n",
      "{'loss': 0.26979517936706543, 'epoch': 53, 'learning_rate': 0.0025, 'validation_loss': 0.26689678430557251}\n",
      "{'loss': 0.26402467489242554, 'epoch': 54, 'learning_rate': 0.0025, 'validation_loss': 0.26117876172065735}\n",
      "{'loss': 0.25835913419723511, 'epoch': 55, 'learning_rate': 0.0025, 'validation_loss': 0.25556626915931702}\n",
      "{'loss': 0.2527996301651001, 'epoch': 56, 'learning_rate': 0.0025, 'validation_loss': 0.25005948543548584}\n",
      "{'loss': 0.24734562635421753, 'epoch': 57, 'learning_rate': 0.0025, 'validation_loss': 0.24465829133987427}\n",
      "{'loss': 0.24199719727039337, 'epoch': 58, 'learning_rate': 0.0025, 'validation_loss': 0.23936267197132111}\n",
      "{'loss': 0.23675444722175598, 'epoch': 59, 'learning_rate': 0.0025, 'validation_loss': 0.23417271673679352}\n",
      "{'loss': 0.23161697387695312, 'epoch': 60, 'learning_rate': 0.0025, 'validation_loss': 0.22908774018287659}\n",
      "{'loss': 0.22658470273017883, 'epoch': 61, 'learning_rate': 0.0025, 'validation_loss': 0.22410780191421509}\n",
      "{'loss': 0.22165709733963013, 'epoch': 62, 'learning_rate': 0.0025, 'validation_loss': 0.2192324697971344}\n",
      "{'loss': 0.21683380007743835, 'epoch': 63, 'learning_rate': 0.0025, 'validation_loss': 0.21446093916893005}\n",
      "{'loss': 0.21211399137973785, 'epoch': 64, 'learning_rate': 0.0025, 'validation_loss': 0.20979286730289459}\n",
      "{'loss': 0.20749747753143311, 'epoch': 65, 'learning_rate': 0.0025, 'validation_loss': 0.2052275538444519}\n",
      "{'loss': 0.20298327505588531, 'epoch': 66, 'learning_rate': 0.0025, 'validation_loss': 0.20076411962509155}\n",
      "{'loss': 0.19857026636600494, 'epoch': 67, 'learning_rate': 0.0025, 'validation_loss': 0.19640159606933594}\n",
      "{'loss': 0.19425773620605469, 'epoch': 68, 'learning_rate': 0.0025, 'validation_loss': 0.19213882088661194}\n",
      "{'loss': 0.19004455208778381, 'epoch': 69, 'learning_rate': 0.0025, 'validation_loss': 0.18797485530376434}\n",
      "{'loss': 0.18592976033687592, 'epoch': 70, 'learning_rate': 0.0025, 'validation_loss': 0.18390873074531555}\n",
      "{'loss': 0.18191182613372803, 'epoch': 71, 'learning_rate': 0.0025, 'validation_loss': 0.17993894219398499}\n",
      "{'loss': 0.17798981070518494, 'epoch': 72, 'learning_rate': 0.0025, 'validation_loss': 0.17606419324874878}\n",
      "{'loss': 0.17416195571422577, 'epoch': 73, 'learning_rate': 0.0025, 'validation_loss': 0.17228317260742188}\n",
      "{'loss': 0.17042736709117889, 'epoch': 74, 'learning_rate': 0.0025, 'validation_loss': 0.16859433054924011}\n",
      "{'loss': 0.16678398847579956, 'epoch': 75, 'learning_rate': 0.0025, 'validation_loss': 0.1649961918592453}\n",
      "{'loss': 0.16323074698448181, 'epoch': 76, 'learning_rate': 0.00125, 'validation_loss': 0.16235747933387756}\n",
      "{'loss': 0.16148991882801056, 'epoch': 77, 'learning_rate': 0.00125, 'validation_loss': 0.16062790155410767}\n",
      "{'loss': 0.15977101027965546, 'epoch': 78, 'learning_rate': 0.00125, 'validation_loss': 0.15891975164413452}\n",
      "{'loss': 0.1580737829208374, 'epoch': 79, 'learning_rate': 0.00125, 'validation_loss': 0.15723335742950439}\n",
      "{'loss': 0.15639796853065491, 'epoch': 80, 'learning_rate': 0.00125, 'validation_loss': 0.15556806325912476}\n",
      "{'loss': 0.15474343299865723, 'epoch': 81, 'learning_rate': 0.00125, 'validation_loss': 0.15392401814460754}\n",
      "{'loss': 0.15310971438884735, 'epoch': 82, 'learning_rate': 0.00125, 'validation_loss': 0.15230070054531097}\n",
      "{'loss': 0.15149688720703125, 'epoch': 83, 'learning_rate': 0.00125, 'validation_loss': 0.15069834887981415}\n",
      "{'loss': 0.14990469813346863, 'epoch': 84, 'learning_rate': 0.00125, 'validation_loss': 0.14911635220050812}\n",
      "{'loss': 0.1483328640460968, 'epoch': 85, 'learning_rate': 0.00125, 'validation_loss': 0.14755451679229736}\n",
      "{'loss': 0.14678117632865906, 'epoch': 86, 'learning_rate': 0.00125, 'validation_loss': 0.14601287245750427}\n",
      "{'loss': 0.14524953067302704, 'epoch': 87, 'learning_rate': 0.00125, 'validation_loss': 0.14449092745780945}\n",
      "{'loss': 0.14373740553855896, 'epoch': 88, 'learning_rate': 0.00125, 'validation_loss': 0.1429886519908905}\n",
      "{'loss': 0.14224487543106079, 'epoch': 89, 'learning_rate': 0.00125, 'validation_loss': 0.14150583744049072}\n",
      "{'loss': 0.14077173173427582, 'epoch': 90, 'learning_rate': 0.00125, 'validation_loss': 0.14004221558570862}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.13931748270988464, 'epoch': 91, 'learning_rate': 0.00125, 'validation_loss': 0.13859753310680389}\n",
      "{'loss': 0.13788230717182159, 'epoch': 92, 'learning_rate': 0.00125, 'validation_loss': 0.13717170059680939}\n",
      "{'loss': 0.13646548986434937, 'epoch': 93, 'learning_rate': 0.00125, 'validation_loss': 0.13576421141624451}\n",
      "{'loss': 0.13506753742694855, 'epoch': 94, 'learning_rate': 0.00125, 'validation_loss': 0.13437503576278687}\n",
      "{'loss': 0.13368743658065796, 'epoch': 95, 'learning_rate': 0.00125, 'validation_loss': 0.13300429284572601}\n",
      "{'loss': 0.13232550024986267, 'epoch': 96, 'learning_rate': 0.00125, 'validation_loss': 0.1316511332988739}\n",
      "{'loss': 0.1309812068939209, 'epoch': 97, 'learning_rate': 0.00125, 'validation_loss': 0.13031572103500366}\n",
      "{'loss': 0.12965463101863861, 'epoch': 98, 'learning_rate': 0.00125, 'validation_loss': 0.12899765372276306}\n",
      "{'loss': 0.12834516167640686, 'epoch': 99, 'learning_rate': 0.00125, 'validation_loss': 0.1276969313621521}\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/MemN2N.model-182\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = MemN2N(FLAGS, sess)\n",
    "    model.build_model()\n",
    "\n",
    "    if FLAGS.is_test:\n",
    "        model.run(valid_stories, valid_questions, test_stories, test_questions)\n",
    "    else:\n",
    "        model.run(train_stories, train_questions, valid_stories, valid_questions)\n",
    "        \n",
    "    predictions, target = model.predict(train_stories, train_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story:\n",
      "[['john', 'travelled', 'to', 'the', 'hallway'],\n",
      " ['김수상', 'journeyed', 'to', 'the', 'bathroom']]\n",
      "\n",
      "Question:\n",
      "['where', 'is', 'john']\n",
      "\n",
      "Prediction:\n",
      "['hallway']\n",
      "\n",
      "Answer:\n",
      "['hallway']\n",
      "\n",
      "Correct:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = 0\n",
    "\n",
    "depad_data(train_stories, train_questions)\n",
    "\n",
    "question = train_questions[index]['question']\n",
    "answer = train_questions[index]['answer']\n",
    "story_index = train_questions[index]['story_index']\n",
    "sentence_index = train_questions[index]['sentence_index']\n",
    "\n",
    "story = train_stories[story_index][:sentence_index + 1]\n",
    "\n",
    "story = [list(map(idx2word.get, sentence)) for sentence in story]\n",
    "question = list(map(idx2word.get, question))\n",
    "prediction = [idx2word[np.argmax(predictions[index])]]\n",
    "answer = list(map(idx2word.get, answer))\n",
    "\n",
    "print('Story:')\n",
    "pp.pprint(story)\n",
    "print('\\nQuestion:')\n",
    "pp.pprint(question)\n",
    "print('\\nPrediction:')\n",
    "pp.pprint(prediction)\n",
    "print('\\nAnswer:')\n",
    "pp.pprint(answer)\n",
    "print('\\nCorrect:')\n",
    "pp.pprint(prediction == answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
