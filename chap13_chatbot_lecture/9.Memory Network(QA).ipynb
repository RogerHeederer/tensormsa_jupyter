{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 13:51:32) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print (sys.version)\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lines= ['1 John travelled to the hallway.',\n",
    "# '2 김수상 journeyed to the bathroom.',\n",
    "# '3 Where is John? \thallway\t1',\n",
    "# '4 Daniel went back to the bathroom.',\n",
    "# '5 John moved to the bedroom.',\n",
    "# '6 Where is 김수상? \tbathroom\t2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to End Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines= ['1 John travelled to the hallway.', \n",
    "'2 김수상 journeyed to the bathroom.',\n",
    "'3 Where is John? \thallway',\n",
    "'4 Daniel went back to the bathroom.',\n",
    "'5 John moved to the bedroom.',\n",
    "'6 Where is 김수상? \tbathroom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력값의 명사를 통해 완전한 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def read_data(fname, word2idx, max_words, max_sentences):\n",
    "    # stories[story_ind] = [[sentence1], [sentence2], ..., [sentenceN]]\n",
    "    # questions[question_ind] = {'question': [question], 'answer': [answer], 'story_index': #, 'sentence_index': #}\n",
    "    stories = dict()\n",
    "    questions = dict()\n",
    "    \n",
    "    \n",
    "    if len(word2idx) == 0:\n",
    "        word2idx['<null>'] = 0\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        max_words = max(max_words, len(words))\n",
    "        \n",
    "        # Determine whether the line indicates the start of a new story\n",
    "        if words[0] == '1':\n",
    "            story_ind = len(stories)\n",
    "            sentence_ind = 0\n",
    "            stories[story_ind] = []\n",
    "        \n",
    "        # Determine whether the line is a question or not\n",
    "        if '?' in line:\n",
    "            is_question = True\n",
    "            question_ind = len(questions)\n",
    "            questions[question_ind] = {'question': [], 'answer': [], 'story_index': story_ind, 'sentence_index': sentence_ind}\n",
    "        else:\n",
    "            is_question = False\n",
    "            sentence_ind = len(stories[story_ind])\n",
    "        \n",
    "        # Parse and append the words to appropriate dictionary / Expand word2idx dictionary\n",
    "        sentence_list = []\n",
    "        for k in range(1, len(words)):\n",
    "            w = words[k].lower()\n",
    "            \n",
    "            # Remove punctuation\n",
    "            if ('.' in w) or ('?' in w):\n",
    "                w = w[:-1]\n",
    "            \n",
    "            # Add new word to dictionary\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = len(word2idx)\n",
    "            \n",
    "            # Append sentence to story dict if not question\n",
    "            if not is_question:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '.' in words[k]:\n",
    "                    stories[story_ind].append(sentence_list)\n",
    "                    break\n",
    "            \n",
    "            # Append sentence and answer to question dict if question\n",
    "            else:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '?' in words[k]:\n",
    "                    answer = words[k + 1].lower()\n",
    "                    \n",
    "                    if answer not in word2idx:\n",
    "                        word2idx[answer] = len(word2idx)\n",
    "                    \n",
    "                    questions[question_ind]['question'].extend(sentence_list)\n",
    "                    questions[question_ind]['answer'].append(answer)\n",
    "                    break\n",
    "        \n",
    "        # Update max_sentences\n",
    "        max_sentences = max(max_sentences, sentence_ind+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert the words into indices\n",
    "    for idx, context in stories.items():\n",
    "        for i in range(len(context)):\n",
    "            temp = list(map(word2idx.get, context[i]))\n",
    "            context[i] = temp\n",
    "    \n",
    "    for idx, value in questions.items():\n",
    "        temp1 = list(map(word2idx.get, value['question']))\n",
    "        temp2 = list(map(word2idx.get, value['answer']))\n",
    "        \n",
    "        value['question'] = temp1\n",
    "        value['answer'] = temp2\n",
    "    \n",
    "    return stories, questions, max_words, max_sentences\n",
    "\n",
    "\n",
    "def pad_data(stories, questions, max_words, max_sentences):\n",
    "\n",
    "    # Pad the context into same size with '<null>'\n",
    "    for idx, context in stories.items():\n",
    "        for sentence in context:           \n",
    "            while len(sentence) < max_words:\n",
    "                sentence.append(0)\n",
    "        while len(context) < max_sentences:\n",
    "            context.append([0] * max_words)\n",
    "    \n",
    "    # Pad the question into same size with '<null>'\n",
    "    for idx, value in questions.items():\n",
    "        while len(value['question']) < max_words:\n",
    "            value['question'].append(0)\n",
    "\n",
    "\n",
    "def depad_data(stories, questions):\n",
    "\n",
    "    for idx, context in stories.items():\n",
    "        for i in range(len(context)):\n",
    "            if 0 in context[i]:\n",
    "                if context[i][0] == 0:\n",
    "                    temp = context[:i]\n",
    "                    context = temp\n",
    "                    break\n",
    "                else:\n",
    "                    index = context[i].index(0)\n",
    "                    context[i] = context[i][:index]\n",
    "\n",
    "    for idx, value in questions.items():\n",
    "        if 0 in value['question']:\n",
    "            index = value['question'].index(0)\n",
    "            value['question'] = value['question'][:index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anneal_epoch': 25,\n",
      " 'anneal_rate': 0.5,\n",
      " 'babi_task': 1,\n",
      " 'batch_size': 32,\n",
      " 'checkpoint_dir': './checkpoints',\n",
      " 'data_dir': './bAbI/en-valid',\n",
      " 'edim': 20,\n",
      " 'init_lr': 0.01,\n",
      " 'init_mean': 0.0,\n",
      " 'init_std': 0.1,\n",
      " 'is_test': False,\n",
      " 'lin_start': False,\n",
      " 'max_grad_norm': 40,\n",
      " 'max_sentences': 4,\n",
      " 'max_words': 7,\n",
      " 'mem_size': 50,\n",
      " 'nepoch': 100,\n",
      " 'nhop': 3,\n",
      " 'nwords': 16,\n",
      " 'show_progress': False}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_integer(\"edim\", 20, \"internal state dimension [20]\")\n",
    "flags.DEFINE_integer(\"nhop\", 3, \"number of hops [3]\")\n",
    "flags.DEFINE_integer(\"mem_size\", 50, \"maximum number of sentences that can be encoded into memory [50]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"batch size to use during training [32]\")\n",
    "flags.DEFINE_integer(\"nepoch\", 100, \"number of epoch to use during training [100]\")\n",
    "flags.DEFINE_integer(\"anneal_epoch\", 25, \"anneal the learning rate every <anneal_epoch> epochs [25]\")\n",
    "flags.DEFINE_integer(\"babi_task\", 1, \"index of bAbI task for the network to learn [1]\")\n",
    "flags.DEFINE_float(\"init_lr\", 0.01, \"initial learning rate [0.01]\")\n",
    "flags.DEFINE_float(\"anneal_rate\", 0.5, \"learning rate annealing rate [0.5]\")\n",
    "flags.DEFINE_float(\"init_mean\", 0., \"weight initialization mean [0.]\")\n",
    "flags.DEFINE_float(\"init_std\", 0.1, \"weight initialization std [0.1]\")\n",
    "flags.DEFINE_float(\"max_grad_norm\", 40, \"clip gradients to this norm [40]\")\n",
    "flags.DEFINE_string(\"data_dir\", \"./bAbI/en-valid\", \"dataset directory [./bAbI/en_valid]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"./checkpoints\", \"checkpoint directory [./checkpoints]\")\n",
    "flags.DEFINE_boolean(\"lin_start\", False, \"True for linear start training, False for otherwise [False]\")\n",
    "flags.DEFINE_boolean(\"is_test\", False, \"True for testing, False for training [False]\")\n",
    "flags.DEFINE_boolean(\"show_progress\", False, \"print progress [False]\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "word2idx = {}\n",
    "max_words = 0\n",
    "max_sentences = 0\n",
    "\n",
    "train_stories, train_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "valid_stories, valid_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "test_stories, test_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "\n",
    "pad_data(train_stories, train_questions, max_words, max_sentences)\n",
    "pad_data(valid_stories, valid_questions, max_words, max_sentences)\n",
    "pad_data(test_stories, test_questions, max_words, max_sentences)\n",
    "\n",
    "idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "FLAGS.nwords = len(word2idx)\n",
    "FLAGS.max_words = max_words\n",
    "FLAGS.max_sentences = max_sentences\n",
    "\n",
    "pp.pprint(flags.FLAGS.__flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ProgressBar(Bar):\n",
    "    message = 'Loading'\n",
    "    fill = '#'\n",
    "    suffix = '%(percent).1f%% | ETA: %(eta)ds'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습결과 출력\n",
    "* Memory Network 학습 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class MemN2N(object):\n",
    "    \n",
    "    def __init__(self, config, sess):\n",
    "        self.nwords = config.nwords\n",
    "        self.max_words = config.max_words\n",
    "        self.max_sentences = config.max_sentences\n",
    "        self.init_mean = config.init_mean\n",
    "        self.init_std = config.init_std\n",
    "        self.batch_size = config.batch_size\n",
    "        self.nepoch = config.nepoch\n",
    "        self.anneal_epoch = config.anneal_epoch\n",
    "        self.nhop = config.nhop\n",
    "        self.edim = config.edim\n",
    "        self.mem_size = config.mem_size\n",
    "        self.max_grad_norm = config.max_grad_norm\n",
    "        \n",
    "        self.lin_start = config.lin_start\n",
    "        self.show_progress = config.show_progress\n",
    "        self.is_test = config.is_test\n",
    "\n",
    "        self.checkpoint_dir = config.checkpoint_dir\n",
    "        \n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        \n",
    "        self.query = tf.placeholder(tf.int32, [None, self.max_words], name='input')\n",
    "        self.time = tf.placeholder(tf.int32, [None, self.mem_size], name='time')\n",
    "        self.target = tf.placeholder(tf.float32, [None, self.nwords], name='target')\n",
    "        self.context = tf.placeholder(tf.int32, [None, self.mem_size, self.max_words], name='context')\n",
    "        \n",
    "        self.hid = []\n",
    "        \n",
    "        self.lr = None\n",
    "        \n",
    "        if self.lin_start:\n",
    "            self.current_lr = 0.005\n",
    "        else:\n",
    "            self.current_lr = config.init_lr\n",
    "\n",
    "        self.anneal_rate = config.anneal_rate\n",
    "        self.loss = None\n",
    "        self.optim = None\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.log_loss = []\n",
    "        self.log_perp = []\n",
    "    \n",
    "    def build_memory(self):\n",
    "        self.global_step = tf.Variable(0, name='global_step')\n",
    "        \n",
    "        zeros = tf.constant(0, tf.float32, [1, self.edim])\n",
    "        self.A_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.B_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.C_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        \n",
    "        A = tf.concat([zeros, self.A_], axis=0)\n",
    "        B = tf.concat([zeros, self.B_], axis=0)\n",
    "        C = tf.concat([zeros, self.C_], axis=0)\n",
    "        \n",
    "        self.T_A_ = tf.Variable(tf.random_normal([self.mem_size - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.T_C_ = tf.Variable(tf.random_normal([self.mem_size - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        \n",
    "        T_A = tf.concat([zeros, self.T_A_], axis=0)\n",
    "        T_C = tf.concat([zeros, self.T_C_], axis=0)\n",
    "        \n",
    "        A_ebd = tf.nn.embedding_lookup(A, self.context)   # [batch_size, mem_size, max_length, edim]\n",
    "        A_ebd = tf.reduce_sum(A_ebd, axis=2)              # [batch_size, mem_size, edim]\n",
    "        T_A_ebd = tf.nn.embedding_lookup(T_A, self.time)  # [batch_size, mem_size, edim]\n",
    "        A_in = tf.add(A_ebd, T_A_ebd)                     # [batch_size, mem_size, edim]\n",
    "        \n",
    "        C_ebd = tf.nn.embedding_lookup(C, self.context)   # [batch_size, mem_size, max_length, edim]\n",
    "        C_ebd = tf.reduce_sum(C_ebd, axis=2)              # [batch_size, mem_size, edim]\n",
    "        T_C_ebd = tf.nn.embedding_lookup(T_C, self.time)  # [batch_size, mem_size, edim]\n",
    "        C_in = tf.add(C_ebd, T_C_ebd)                     # [batch_size, mem_size, edim]\n",
    "        \n",
    "        query_ebd = tf.nn.embedding_lookup(B, self.query) # [batch_size, max_length, edim]\n",
    "        query_ebd = tf.reduce_sum(query_ebd, axis=1)      # [batch_size, edim]\n",
    "        self.hid.append(query_ebd)\n",
    "        \n",
    "        for h in range(self.nhop):\n",
    "            q3dim = tf.reshape(self.hid[-1], [-1, 1, self.edim]) # [batch_size, edim] ==> [batch_size, 1, edim]\n",
    "            p3dim = tf.matmul(q3dim, A_in, transpose_b=True)     # [batch_size, 1, edim] X [batch_size, edim, mem_size]\n",
    "            p2dim = tf.reshape(p3dim, [-1, self.mem_size])       # [batch_size, mem_size]\n",
    "            \n",
    "            # If linear start, remove softmax layers\n",
    "            if self.lin_start:\n",
    "                p = p2dim\n",
    "            else:\n",
    "                p = tf.nn.softmax(p2dim)\n",
    "            \n",
    "            p3dim = tf.reshape(p, [-1, 1, self.mem_size]) # [batch_size, 1, mem_size]\n",
    "            o3dim = tf.matmul(p3dim, C_in)                # [batch_size, 1, mem_size] X [batch_size, mem_size, edim]\n",
    "            o2dim = tf.reshape(o3dim, [-1, self.edim])    # [batch_size, edim]\n",
    "            \n",
    "            a = tf.add(o2dim, self.hid[-1]) # [batch_size, edim]\n",
    "            self.hid.append(a)              # [input, a_1, a_2, ..., a_nhop]\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.build_memory()\n",
    "        \n",
    "        self.W = tf.Variable(tf.random_normal([self.edim, self.nwords], mean=self.init_mean, stddev=self.init_std))\n",
    "        a_hat = tf.matmul(self.hid[-1], self.W)\n",
    "        \n",
    "        self.hypothesis = tf.nn.softmax(a_hat)\n",
    "\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=a_hat, labels=self.target)\n",
    "        \n",
    "        self.lr = tf.Variable(self.current_lr)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        \n",
    "        params = [self.A_, self.B_, self.C_, self.T_A_, self.T_C_, self.W]\n",
    "        grads_and_vars = self.opt.compute_gradients(self.loss, params)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], self.max_grad_norm), gv[1]) for gv in grads_and_vars]\n",
    "        \n",
    "        inc = self.global_step.assign_add(1)\n",
    "        with tf.control_dependencies([inc]):\n",
    "            self.optim = self.opt.apply_gradients(clipped_grads_and_vars)\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    def train(self, train_stories, train_questions):\n",
    "        N = int(math.ceil(len(train_questions) / self.batch_size))\n",
    "        cost = 0\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "        \n",
    "        for idx in range(N):\n",
    "            \n",
    "            if self.show_progress:\n",
    "                bar.next()\n",
    "            \n",
    "            if idx == N - 1:\n",
    "                iterations = len(train_questions) - (N - 1) * self.batch_size\n",
    "            else:\n",
    "                iterations = self.batch_size\n",
    "            \n",
    "            query = np.ndarray([iterations, self.max_words], dtype=np.int32)\n",
    "            time = np.zeros([iterations, self.mem_size], dtype=np.int32)\n",
    "            target = np.zeros([iterations, self.nwords], dtype=np.float32)\n",
    "            context = np.ndarray([iterations, self.mem_size, self.max_words], dtype=np.int32)\n",
    "            \n",
    "            for b in range(iterations):\n",
    "                m = idx * self.batch_size + b\n",
    "                \n",
    "                curr_q = train_questions[m]\n",
    "                q_text = curr_q['question']\n",
    "                story_ind = curr_q['story_index']\n",
    "                sent_ind = curr_q['sentence_index']\n",
    "                answer = curr_q['answer'][0]\n",
    "                \n",
    "                curr_s = train_stories[story_ind]\n",
    "                curr_c = curr_s[:sent_ind + 1]\n",
    "\n",
    "                if len(curr_c) >= self.mem_size:\n",
    "                    curr_c = curr_c[-self.mem_size:]\n",
    "                    \n",
    "                    for t in range(self.mem_size):\n",
    "                        time[b, t].fill(t)\n",
    "                else:\n",
    "                    \n",
    "                    for t in range(len(curr_c)):\n",
    "                        time[b, t].fill(t)\n",
    "                    \n",
    "                    while len(curr_c) < self.mem_size:\n",
    "                        curr_c.append([0.] * self.max_words)\n",
    "                \n",
    "                query[b, :] = q_text\n",
    "                target[b, answer] = 1\n",
    "                context[b, :, :] = curr_c\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim, self.loss, self.global_step],\n",
    "                                               feed_dict={self.query: query, self.time: time,\n",
    "                                                          self.target: target, self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar.finish()\n",
    "        \n",
    "        return cost / len(train_questions)\n",
    "    \n",
    "    \n",
    "    def test(self, test_stories, test_questions, label='Test'):\n",
    "        N = int(math.ceil(len(test_questions) / self.batch_size))\n",
    "        cost = 0\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "        \n",
    "        for idx in range(N):\n",
    "            \n",
    "            if self.show_progress:\n",
    "                bar.next()\n",
    "            \n",
    "            if idx == N - 1:\n",
    "                iterations = len(test_questions) - (N - 1) * self.batch_size\n",
    "            else:\n",
    "                iterations = self.batch_size\n",
    "            \n",
    "            query = np.ndarray([iterations, self.max_words], dtype=np.int32)\n",
    "            time = np.zeros([iterations, self.mem_size], dtype=np.int32)\n",
    "            target = np.zeros([iterations, self.nwords], dtype=np.float32)\n",
    "            context = np.ndarray([iterations, self.mem_size, self.max_words], dtype=np.int32)\n",
    "            \n",
    "            for b in range(iterations):\n",
    "                m = idx * self.batch_size + b\n",
    "                \n",
    "                curr_q = test_questions[m]\n",
    "                q_text = curr_q['question']\n",
    "                story_ind = curr_q['story_index']\n",
    "                sent_ind = curr_q['sentence_index']\n",
    "                answer = curr_q['answer'][0]\n",
    "                \n",
    "                curr_s = test_stories[story_ind]\n",
    "                curr_c = curr_s[:sent_ind + 1]\n",
    "                \n",
    "                if len(curr_c) >= self.mem_size:\n",
    "                    curr_c = curr_c[-self.mem_size:]\n",
    "                    \n",
    "                    for t in range(self.mem_size):\n",
    "                        time[b, t].fill(t)\n",
    "                else:\n",
    "                    \n",
    "                    for t in range(len(curr_c)):\n",
    "                        time[b, t].fill(t)\n",
    "                    \n",
    "                    while len(curr_c) < self.mem_size:\n",
    "                        curr_c.append([0.] * self.max_words)\n",
    "                \n",
    "                query[b, :] = q_text\n",
    "                target[b, answer] = 1\n",
    "                context[b, :, :] = curr_c\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim, self.loss, self.global_step],\n",
    "                                               feed_dict={self.query: query, self.time: time,\n",
    "                                                          self.target: target, self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar.finish()\n",
    "        \n",
    "        return cost / len(test_questions)\n",
    "    \n",
    "    \n",
    "    def run(self, train_stories, train_questions, test_stories, test_questions):\n",
    "        if not self.is_test:# add not\n",
    "\n",
    "            for idx in range(self.nepoch):\n",
    "                train_loss = np.sum(self.train(train_stories, train_questions))\n",
    "                test_loss = np.sum(self.test(test_stories, test_questions, label='Validation'))\n",
    "                \n",
    "                self.log_loss.append([train_loss, test_loss])\n",
    "                \n",
    "                state = {\n",
    "                    'loss': train_loss,\n",
    "                    'epoch': idx,\n",
    "                    'learning_rate': self.current_lr,\n",
    "                    'validation_loss': test_loss\n",
    "                }\n",
    "                \n",
    "                print(state)\n",
    "                \n",
    "                \n",
    "                # learning rate annealing\n",
    "                if (not idx == 0) and (idx % self.anneal_epoch == 0):\n",
    "                    self.current_lr = self.current_lr * self.anneal_rate\n",
    "                    self.lr.assign(self.current_lr).eval()\n",
    "            \n",
    "                # If validation loss stops decreasing, insert softmax layers\n",
    "                if idx == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    if self.log_loss[idx][1] > self.log_loss[idx - 1][1]:\n",
    "                        self.lin_start = False\n",
    "\n",
    "                if idx % 10 == 0:\n",
    "                    self.saver.save(self.sess,\n",
    "                                    os.path.join(self.checkpoint_dir, \"MemN2N.model\"),\n",
    "                                    global_step=self.step.astype(int))\n",
    "        else:\n",
    "            self.load()\n",
    "            \n",
    "            valid_loss = np.sum(self.test(train_stories, train_questions, label='Validation'))\n",
    "            test_loss = np.sum(self.test(test_stories, test_questions, label='Test'))\n",
    "            \n",
    "            state = {\n",
    "                'validation_loss': valid_loss,\n",
    "                'test_loss': test_loss\n",
    "            }\n",
    "            \n",
    "            print(state)\n",
    "\n",
    "\n",
    "    def predict(self, test_stories, test_questions):\n",
    "        self.load()\n",
    "\n",
    "        num_instances = len(test_questions)\n",
    "\n",
    "        query = np.ndarray([num_instances, self.max_words], dtype=np.int32)\n",
    "        time = np.zeros([num_instances, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([num_instances, self.nwords], dtype=np.float32)\n",
    "        context = np.ndarray([num_instances, self.mem_size, self.max_words], dtype=np.int32)\n",
    "\n",
    "        for b in range(num_instances):\n",
    "            \n",
    "            curr_q = test_questions[b]\n",
    "            q_text = curr_q['question']\n",
    "            story_ind = curr_q['story_index']\n",
    "            sent_ind = curr_q['sentence_index']\n",
    "            answer = curr_q['answer'][0]\n",
    "            \n",
    "            curr_s = test_stories[story_ind]\n",
    "            curr_c = curr_s[:sent_ind + 1]\n",
    "            \n",
    "            if len(curr_c) >= self.mem_size:\n",
    "                curr_c = curr_c[-self.mem_size:]\n",
    "                \n",
    "                for t in range(self.mem_size):\n",
    "                    time[b, t].fill(t)\n",
    "            else:\n",
    "                \n",
    "                for t in range(len(curr_c)):\n",
    "                    time[b, t].fill(t)\n",
    "                \n",
    "                while len(curr_c) < self.mem_size:\n",
    "                    curr_c.append([0.] * self.max_words)\n",
    "            \n",
    "            query[b, :] = q_text\n",
    "            target[b, answer] = 1\n",
    "            context[b, :, :] = curr_c\n",
    "\n",
    "        predictions = self.sess.run(self.hypothesis, feed_dict={self.query: query, self.time: time, self.context: context})\n",
    "\n",
    "        return predictions, target\n",
    "\n",
    "\n",
    "        \n",
    "    def load(self):\n",
    "        print(' [*] Reading checkpoints...')\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\" [!] No checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7341480255126953, 'epoch': 0, 'learning_rate': 0.01, 'validation_loss': 2.7188739776611328}\n",
      "{'loss': 2.7036645412445068, 'epoch': 1, 'learning_rate': 0.01, 'validation_loss': 2.6884958744049072}\n",
      "{'loss': 2.6733448505401611, 'epoch': 2, 'learning_rate': 0.01, 'validation_loss': 2.6581878662109375}\n",
      "{'loss': 2.6430025100708008, 'epoch': 3, 'learning_rate': 0.01, 'validation_loss': 2.6277661323547363}\n",
      "{'loss': 2.6124577522277832, 'epoch': 4, 'learning_rate': 0.01, 'validation_loss': 2.5970544815063477}\n",
      "{'loss': 2.5815362930297852, 'epoch': 5, 'learning_rate': 0.01, 'validation_loss': 2.5658819675445557}\n",
      "{'loss': 2.5500702857971191, 'epoch': 6, 'learning_rate': 0.01, 'validation_loss': 2.5340812206268311}\n",
      "{'loss': 2.5178942680358887, 'epoch': 7, 'learning_rate': 0.01, 'validation_loss': 2.5014896392822266}\n",
      "{'loss': 2.4848480224609375, 'epoch': 8, 'learning_rate': 0.01, 'validation_loss': 2.467949390411377}\n",
      "{'loss': 2.4507746696472168, 'epoch': 9, 'learning_rate': 0.01, 'validation_loss': 2.4333047866821289}\n",
      "{'loss': 2.4155211448669434, 'epoch': 10, 'learning_rate': 0.01, 'validation_loss': 2.3974056243896484}\n",
      "{'loss': 2.3789396286010742, 'epoch': 11, 'learning_rate': 0.01, 'validation_loss': 2.3601055145263672}\n",
      "{'loss': 2.340886116027832, 'epoch': 12, 'learning_rate': 0.01, 'validation_loss': 2.3212642669677734}\n",
      "{'loss': 2.3012242317199707, 'epoch': 13, 'learning_rate': 0.01, 'validation_loss': 2.2807490825653076}\n",
      "{'loss': 2.2598245143890381, 'epoch': 14, 'learning_rate': 0.01, 'validation_loss': 2.2384352684020996}\n",
      "{'loss': 2.2165679931640625, 'epoch': 15, 'learning_rate': 0.01, 'validation_loss': 2.194209098815918}\n",
      "{'loss': 2.1713471412658691, 'epoch': 16, 'learning_rate': 0.01, 'validation_loss': 2.1479706764221191}\n",
      "{'loss': 2.1240699291229248, 'epoch': 17, 'learning_rate': 0.01, 'validation_loss': 2.0996365547180176}\n",
      "{'loss': 2.0746631622314453, 'epoch': 18, 'learning_rate': 0.01, 'validation_loss': 2.0491440296173096}\n",
      "{'loss': 2.0230753421783447, 'epoch': 19, 'learning_rate': 0.01, 'validation_loss': 1.9964542388916016}\n",
      "{'loss': 1.9692813158035278, 'epoch': 20, 'learning_rate': 0.01, 'validation_loss': 1.9415575265884399}\n",
      "{'loss': 1.9132869243621826, 'epoch': 21, 'learning_rate': 0.01, 'validation_loss': 1.8844759464263916}\n",
      "{'loss': 1.8551328182220459, 'epoch': 22, 'learning_rate': 0.01, 'validation_loss': 1.8252692222595215}\n",
      "{'loss': 1.7948987483978271, 'epoch': 23, 'learning_rate': 0.01, 'validation_loss': 1.7640379667282104}\n",
      "{'loss': 1.73270583152771, 'epoch': 24, 'learning_rate': 0.01, 'validation_loss': 1.7009248733520508}\n",
      "{'loss': 1.6687190532684326, 'epoch': 25, 'learning_rate': 0.01, 'validation_loss': 1.6361165046691895}\n",
      "{'loss': 1.6031467914581299, 'epoch': 26, 'learning_rate': 0.005, 'validation_loss': 1.5865136384963989}\n",
      "{'loss': 1.5698015689849854, 'epoch': 27, 'learning_rate': 0.005, 'validation_loss': 1.5530149936676025}\n",
      "{'loss': 1.5361584424972534, 'epoch': 28, 'learning_rate': 0.005, 'validation_loss': 1.5192365646362305}\n",
      "{'loss': 1.5022540092468262, 'epoch': 29, 'learning_rate': 0.005, 'validation_loss': 1.4852159023284912}\n",
      "{'loss': 1.4681270122528076, 'epoch': 30, 'learning_rate': 0.005, 'validation_loss': 1.4509919881820679}\n",
      "{'loss': 1.4338163137435913, 'epoch': 31, 'learning_rate': 0.005, 'validation_loss': 1.4166051149368286}\n",
      "{'loss': 1.3993630409240723, 'epoch': 32, 'learning_rate': 0.005, 'validation_loss': 1.3820955753326416}\n",
      "{'loss': 1.3648078441619873, 'epoch': 33, 'learning_rate': 0.005, 'validation_loss': 1.3475046157836914}\n",
      "{'loss': 1.3301913738250732, 'epoch': 34, 'learning_rate': 0.005, 'validation_loss': 1.3128731250762939}\n",
      "{'loss': 1.2955547571182251, 'epoch': 35, 'learning_rate': 0.005, 'validation_loss': 1.2782412767410278}\n",
      "{'loss': 1.2609376907348633, 'epoch': 36, 'learning_rate': 0.005, 'validation_loss': 1.2436487674713135}\n",
      "{'loss': 1.2263786792755127, 'epoch': 37, 'learning_rate': 0.005, 'validation_loss': 1.2091330289840698}\n",
      "{'loss': 1.1919151544570923, 'epoch': 38, 'learning_rate': 0.005, 'validation_loss': 1.1747304201126099}\n",
      "{'loss': 1.1575824022293091, 'epoch': 39, 'learning_rate': 0.005, 'validation_loss': 1.1404755115509033}\n",
      "{'loss': 1.1234139204025269, 'epoch': 40, 'learning_rate': 0.005, 'validation_loss': 1.1064009666442871}\n",
      "{'loss': 1.0894408226013184, 'epoch': 41, 'learning_rate': 0.005, 'validation_loss': 1.0725365877151489}\n",
      "{'loss': 1.0556924343109131, 'epoch': 42, 'learning_rate': 0.005, 'validation_loss': 1.0389113426208496}\n",
      "{'loss': 1.0221965312957764, 'epoch': 43, 'learning_rate': 0.005, 'validation_loss': 1.0055514574050903}\n",
      "{'loss': 0.9889792799949646, 'epoch': 44, 'learning_rate': 0.005, 'validation_loss': 0.97248286008834839}\n",
      "{'loss': 0.95606565475463867, 'epoch': 45, 'learning_rate': 0.005, 'validation_loss': 0.93973064422607422}\n",
      "{'loss': 0.92348116636276245, 'epoch': 46, 'learning_rate': 0.005, 'validation_loss': 0.90732026100158691}\n",
      "{'loss': 0.8912513256072998, 'epoch': 47, 'learning_rate': 0.005, 'validation_loss': 0.87527817487716675}\n",
      "{'loss': 0.85940420627593994, 'epoch': 48, 'learning_rate': 0.005, 'validation_loss': 0.84363371133804321}\n",
      "{'loss': 0.82797062397003174, 'epoch': 49, 'learning_rate': 0.005, 'validation_loss': 0.81241977214813232}\n",
      "{'loss': 0.79698556661605835, 'epoch': 50, 'learning_rate': 0.005, 'validation_loss': 0.7816733717918396}\n",
      "{'loss': 0.76648890972137451, 'epoch': 51, 'learning_rate': 0.0025, 'validation_loss': 0.75895464420318604}\n",
      "{'loss': 0.75145459175109863, 'epoch': 52, 'learning_rate': 0.0025, 'validation_loss': 0.74398940801620483}\n",
      "{'loss': 0.73655974864959717, 'epoch': 53, 'learning_rate': 0.0025, 'validation_loss': 0.72916698455810547}\n",
      "{'loss': 0.72181159257888794, 'epoch': 54, 'learning_rate': 0.0025, 'validation_loss': 0.71449482440948486}\n",
      "{'loss': 0.70721709728240967, 'epoch': 55, 'learning_rate': 0.0025, 'validation_loss': 0.69997984170913696}\n",
      "{'loss': 0.69278377294540405, 'epoch': 56, 'learning_rate': 0.0025, 'validation_loss': 0.68563008308410645}\n",
      "{'loss': 0.67851972579956055, 'epoch': 57, 'learning_rate': 0.0025, 'validation_loss': 0.67145359516143799}\n",
      "{'loss': 0.66443294286727905, 'epoch': 58, 'learning_rate': 0.0025, 'validation_loss': 0.65745842456817627}\n",
      "{'loss': 0.6505315899848938, 'epoch': 59, 'learning_rate': 0.0025, 'validation_loss': 0.64365315437316895}\n",
      "{'loss': 0.63682436943054199, 'epoch': 60, 'learning_rate': 0.0025, 'validation_loss': 0.63004601001739502}\n",
      "{'loss': 0.62331938743591309, 'epoch': 61, 'learning_rate': 0.0025, 'validation_loss': 0.6166454553604126}\n",
      "{'loss': 0.61002516746520996, 'epoch': 62, 'learning_rate': 0.0025, 'validation_loss': 0.60345941781997681}\n",
      "{'loss': 0.59694921970367432, 'epoch': 63, 'learning_rate': 0.0025, 'validation_loss': 0.59049588441848755}\n",
      "{'loss': 0.58409976959228516, 'epoch': 64, 'learning_rate': 0.0025, 'validation_loss': 0.57776212692260742}\n",
      "{'loss': 0.5714837908744812, 'epoch': 65, 'learning_rate': 0.0025, 'validation_loss': 0.56526541709899902}\n",
      "{'loss': 0.55910778045654297, 'epoch': 66, 'learning_rate': 0.0025, 'validation_loss': 0.55301165580749512}\n",
      "{'loss': 0.54697763919830322, 'epoch': 67, 'learning_rate': 0.0025, 'validation_loss': 0.54100656509399414}\n",
      "{'loss': 0.53509879112243652, 'epoch': 68, 'learning_rate': 0.0025, 'validation_loss': 0.5292549729347229}\n",
      "{'loss': 0.5234755277633667, 'epoch': 69, 'learning_rate': 0.0025, 'validation_loss': 0.51776081323623657}\n",
      "{'loss': 0.51211118698120117, 'epoch': 70, 'learning_rate': 0.0025, 'validation_loss': 0.50652694702148438}\n",
      "{'loss': 0.50100880861282349, 'epoch': 71, 'learning_rate': 0.0025, 'validation_loss': 0.49555587768554688}\n",
      "{'loss': 0.49016952514648438, 'epoch': 72, 'learning_rate': 0.0025, 'validation_loss': 0.48484927415847778}\n",
      "{'loss': 0.47959467768669128, 'epoch': 73, 'learning_rate': 0.0025, 'validation_loss': 0.47440674901008606}\n",
      "{'loss': 0.46928471326828003, 'epoch': 74, 'learning_rate': 0.0025, 'validation_loss': 0.46422865986824036}\n",
      "{'loss': 0.45923840999603271, 'epoch': 75, 'learning_rate': 0.0025, 'validation_loss': 0.45431375503540039}\n",
      "{'loss': 0.44945460557937622, 'epoch': 76, 'learning_rate': 0.00125, 'validation_loss': 0.447053462266922}\n",
      "{'loss': 0.44466856122016907, 'epoch': 77, 'learning_rate': 0.00125, 'validation_loss': 0.44229960441589355}\n",
      "{'loss': 0.43994709849357605, 'epoch': 78, 'learning_rate': 0.00125, 'validation_loss': 0.43761062622070312}\n",
      "{'loss': 0.4352898895740509, 'epoch': 79, 'learning_rate': 0.00125, 'validation_loss': 0.43298545479774475}\n",
      "{'loss': 0.43069690465927124, 'epoch': 80, 'learning_rate': 0.00125, 'validation_loss': 0.42842411994934082}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.42616736888885498, 'epoch': 81, 'learning_rate': 0.00125, 'validation_loss': 0.42392605543136597}\n",
      "{'loss': 0.42170068621635437, 'epoch': 82, 'learning_rate': 0.00125, 'validation_loss': 0.41949087381362915}\n",
      "{'loss': 0.41729697585105896, 'epoch': 83, 'learning_rate': 0.00125, 'validation_loss': 0.41511827707290649}\n",
      "{'loss': 0.41295543313026428, 'epoch': 84, 'learning_rate': 0.00125, 'validation_loss': 0.41080790758132935}\n",
      "{'loss': 0.40867564082145691, 'epoch': 85, 'learning_rate': 0.00125, 'validation_loss': 0.40655872225761414}\n",
      "{'loss': 0.40445700287818909, 'epoch': 86, 'learning_rate': 0.00125, 'validation_loss': 0.40237072110176086}\n",
      "{'loss': 0.40029963850975037, 'epoch': 87, 'learning_rate': 0.00125, 'validation_loss': 0.39824312925338745}\n",
      "{'loss': 0.39620187878608704, 'epoch': 88, 'learning_rate': 0.00125, 'validation_loss': 0.39417558908462524}\n",
      "{'loss': 0.39216411113739014, 'epoch': 89, 'learning_rate': 0.00125, 'validation_loss': 0.39016750454902649}\n",
      "{'loss': 0.38818556070327759, 'epoch': 90, 'learning_rate': 0.00125, 'validation_loss': 0.38621804118156433}\n",
      "{'loss': 0.38426545262336731, 'epoch': 91, 'learning_rate': 0.00125, 'validation_loss': 0.38232725858688354}\n",
      "{'loss': 0.38040345907211304, 'epoch': 92, 'learning_rate': 0.00125, 'validation_loss': 0.37849396467208862}\n",
      "{'loss': 0.37659907341003418, 'epoch': 93, 'learning_rate': 0.00125, 'validation_loss': 0.3747180700302124}\n",
      "{'loss': 0.37285143136978149, 'epoch': 94, 'learning_rate': 0.00125, 'validation_loss': 0.37099876999855042}\n",
      "{'loss': 0.36916005611419678, 'epoch': 95, 'learning_rate': 0.00125, 'validation_loss': 0.36733555793762207}\n",
      "{'loss': 0.3655247688293457, 'epoch': 96, 'learning_rate': 0.00125, 'validation_loss': 0.36372765898704529}\n",
      "{'loss': 0.36194425821304321, 'epoch': 97, 'learning_rate': 0.00125, 'validation_loss': 0.36017444729804993}\n",
      "{'loss': 0.35841822624206543, 'epoch': 98, 'learning_rate': 0.00125, 'validation_loss': 0.35667562484741211}\n",
      "{'loss': 0.35494637489318848, 'epoch': 99, 'learning_rate': 0.00125, 'validation_loss': 0.35323032736778259}\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/MemN2N.model-182\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = MemN2N(FLAGS, sess)\n",
    "    model.build_model()\n",
    "\n",
    "    if FLAGS.is_test:\n",
    "        model.run(valid_stories, valid_questions, test_stories, test_questions)\n",
    "    else:\n",
    "        model.run(train_stories, train_questions, valid_stories, valid_questions)\n",
    "        \n",
    "    predictions, target = model.predict(train_stories, train_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story:\n",
      "[['john', 'travelled', 'to', 'the', 'hallway'],\n",
      " ['김수상', 'journeyed', 'to', 'the', 'bathroom']]\n",
      "\n",
      "Question:\n",
      "['where', 'is', 'john']\n",
      "\n",
      "Prediction:\n",
      "['hallway']\n",
      "\n",
      "Answer:\n",
      "['hallway']\n",
      "\n",
      "Correct:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = 0\n",
    "\n",
    "depad_data(train_stories, train_questions)\n",
    "\n",
    "question = train_questions[index]['question']\n",
    "answer = train_questions[index]['answer']\n",
    "story_index = train_questions[index]['story_index']\n",
    "sentence_index = train_questions[index]['sentence_index']\n",
    "\n",
    "story = train_stories[story_index][:sentence_index + 1]\n",
    "\n",
    "story = [list(map(idx2word.get, sentence)) for sentence in story]\n",
    "question = list(map(idx2word.get, question))\n",
    "prediction = [idx2word[np.argmax(predictions[index])]]\n",
    "answer = list(map(idx2word.get, answer))\n",
    "\n",
    "print('Story:')\n",
    "pp.pprint(story)\n",
    "print('\\nQuestion:')\n",
    "pp.pprint(question)\n",
    "print('\\nPrediction:')\n",
    "pp.pprint(prediction)\n",
    "print('\\nAnswer:')\n",
    "pp.pprint(answer)\n",
    "print('\\nCorrect:')\n",
    "pp.pprint(prediction == answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
