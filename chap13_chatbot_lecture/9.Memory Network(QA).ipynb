{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Network\n",
    "* reference : https://github.com/carpedm20/MemN2N-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 13:51:32) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print (sys.version)\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stroy Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lines= ['1 John travelled to the hallway.',\n",
    "# '2 김수상 journeyed to the 판교.',\n",
    "# '3 Where is John? \thallway\t1',\n",
    "# '4 Daniel went back to the 판교.',\n",
    "# '5 John moved to the bedroom.',\n",
    "# '6 Where is 김수상? \t판교\t2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to End Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines= ['1 John travelled to the hallway.', \n",
    "'2 김수상 journeyed to the 판교.',\n",
    "'3 Where is John? \thallway',\n",
    "'4 Daniel went back to the 판교.',\n",
    "'5 John moved to the bedroom.',\n",
    "'6 Where is 김수상? \t판교']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력값의 명사를 통해 완전한 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def read_data(fname, word2idx, max_words, max_sentences):\n",
    "    # stories[story_ind] = [[sentence1], [sentence2], ..., [sentenceN]]\n",
    "    # questions[question_ind] = {'question': [question], 'answer': [answer], 'story_index': #, 'sentence_index': #}\n",
    "    stories = dict()\n",
    "    questions = dict()\n",
    "    \n",
    "    \n",
    "    if len(word2idx) == 0:\n",
    "        word2idx['<null>'] = 0\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        max_words = max(max_words, len(words))\n",
    "        \n",
    "        # Determine whether the line indicates the start of a new story\n",
    "        if words[0] == '1':\n",
    "            story_ind = len(stories)\n",
    "            sentence_ind = 0\n",
    "            stories[story_ind] = []\n",
    "        \n",
    "        # Determine whether the line is a question or not\n",
    "        if '?' in line:\n",
    "            is_question = True\n",
    "            question_ind = len(questions)\n",
    "            questions[question_ind] = {'question': [], 'answer': [], 'story_index': story_ind, 'sentence_index': sentence_ind}\n",
    "        else:\n",
    "            is_question = False\n",
    "            sentence_ind = len(stories[story_ind])\n",
    "        \n",
    "        # Parse and append the words to appropriate dictionary / Expand word2idx dictionary\n",
    "        sentence_list = []\n",
    "        for k in range(1, len(words)):\n",
    "            w = words[k].lower()\n",
    "            \n",
    "            # Remove punctuation\n",
    "            if ('.' in w) or ('?' in w):\n",
    "                w = w[:-1]\n",
    "            \n",
    "            # Add new word to dictionary\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = len(word2idx)\n",
    "            \n",
    "            # Append sentence to story dict if not question\n",
    "            if not is_question:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '.' in words[k]:\n",
    "                    stories[story_ind].append(sentence_list)\n",
    "                    break\n",
    "            \n",
    "            # Append sentence and answer to question dict if question\n",
    "            else:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '?' in words[k]:\n",
    "                    answer = words[k + 1].lower()\n",
    "                    \n",
    "                    if answer not in word2idx:\n",
    "                        word2idx[answer] = len(word2idx)\n",
    "                    \n",
    "                    questions[question_ind]['question'].extend(sentence_list)\n",
    "                    questions[question_ind]['answer'].append(answer)\n",
    "                    break\n",
    "        \n",
    "        # Update max_sentences\n",
    "        max_sentences = max(max_sentences, sentence_ind+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert the words into indices\n",
    "    for idx, context in stories.items():\n",
    "        for i in range(len(context)):\n",
    "            temp = list(map(word2idx.get, context[i]))\n",
    "            context[i] = temp\n",
    "    \n",
    "    for idx, value in questions.items():\n",
    "        temp1 = list(map(word2idx.get, value['question']))\n",
    "        temp2 = list(map(word2idx.get, value['answer']))\n",
    "        \n",
    "        value['question'] = temp1\n",
    "        value['answer'] = temp2\n",
    "    \n",
    "    return stories, questions, max_words, max_sentences\n",
    "\n",
    "\n",
    "def pad_data(stories, questions, max_words, max_sentences):\n",
    "\n",
    "    # Pad the context into same size with '<null>'\n",
    "    for idx, context in stories.items():\n",
    "        for sentence in context:           \n",
    "            while len(sentence) < max_words:\n",
    "                sentence.append(0)\n",
    "        while len(context) < max_sentences:\n",
    "            context.append([0] * max_words)\n",
    "    \n",
    "    # Pad the question into same size with '<null>'\n",
    "    for idx, value in questions.items():\n",
    "        while len(value['question']) < max_words:\n",
    "            value['question'].append(0)\n",
    "\n",
    "\n",
    "def depad_data(stories, questions):\n",
    "\n",
    "    for idx, context in stories.items():\n",
    "        for i in range(len(context)):\n",
    "            if 0 in context[i]:\n",
    "                if context[i][0] == 0:\n",
    "                    temp = context[:i]\n",
    "                    context = temp\n",
    "                    break\n",
    "                else:\n",
    "                    index = context[i].index(0)\n",
    "                    context[i] = context[i][:index]\n",
    "\n",
    "    for idx, value in questions.items():\n",
    "        if 0 in value['question']:\n",
    "            index = value['question'].index(0)\n",
    "            value['question'] = value['question'][:index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anneal_epoch': 25,\n",
      " 'anneal_rate': 0.5,\n",
      " 'babi_task': 1,\n",
      " 'batch_size': 32,\n",
      " 'checkpoint_dir': './checkpoints',\n",
      " 'data_dir': './bAbI/en-valid',\n",
      " 'edim': 20,\n",
      " 'init_lr': 0.01,\n",
      " 'init_mean': 0.0,\n",
      " 'init_std': 0.1,\n",
      " 'is_test': False,\n",
      " 'lin_start': False,\n",
      " 'max_grad_norm': 40,\n",
      " 'max_sentences': 4,\n",
      " 'max_words': 7,\n",
      " 'mem_size': 50,\n",
      " 'nepoch': 100,\n",
      " 'nhop': 3,\n",
      " 'nwords': 16,\n",
      " 'show_progress': False}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_integer(\"edim\", 20, \"internal state dimension [20]\")\n",
    "flags.DEFINE_integer(\"nhop\", 3, \"number of hops [3]\")\n",
    "flags.DEFINE_integer(\"mem_size\", 50, \"maximum number of sentences that can be encoded into memory [50]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"batch size to use during training [32]\")\n",
    "flags.DEFINE_integer(\"nepoch\", 100, \"number of epoch to use during training [100]\")\n",
    "flags.DEFINE_integer(\"anneal_epoch\", 25, \"anneal the learning rate every <anneal_epoch> epochs [25]\")\n",
    "flags.DEFINE_integer(\"babi_task\", 1, \"index of bAbI task for the network to learn [1]\")\n",
    "flags.DEFINE_float(\"init_lr\", 0.01, \"initial learning rate [0.01]\")\n",
    "flags.DEFINE_float(\"anneal_rate\", 0.5, \"learning rate annealing rate [0.5]\")\n",
    "flags.DEFINE_float(\"init_mean\", 0., \"weight initialization mean [0.]\")\n",
    "flags.DEFINE_float(\"init_std\", 0.1, \"weight initialization std [0.1]\")\n",
    "flags.DEFINE_float(\"max_grad_norm\", 40, \"clip gradients to this norm [40]\")\n",
    "flags.DEFINE_string(\"data_dir\", \"./bAbI/en-valid\", \"dataset directory [./bAbI/en_valid]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"./checkpoints\", \"checkpoint directory [./checkpoints]\")\n",
    "flags.DEFINE_boolean(\"lin_start\", False, \"True for linear start training, False for otherwise [False]\")\n",
    "flags.DEFINE_boolean(\"is_test\", False, \"True for testing, False for training [False]\")\n",
    "flags.DEFINE_boolean(\"show_progress\", False, \"print progress [False]\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "word2idx = {}\n",
    "max_words = 0\n",
    "max_sentences = 0\n",
    "\n",
    "train_stories, train_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "valid_stories, valid_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "test_stories, test_questions, max_words, max_sentences = read_data(lines, word2idx, max_words, max_sentences)\n",
    "\n",
    "pad_data(train_stories, train_questions, max_words, max_sentences)\n",
    "pad_data(valid_stories, valid_questions, max_words, max_sentences)\n",
    "pad_data(test_stories, test_questions, max_words, max_sentences)\n",
    "\n",
    "idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "FLAGS.nwords = len(word2idx)\n",
    "FLAGS.max_words = max_words\n",
    "FLAGS.max_sentences = max_sentences\n",
    "\n",
    "pp.pprint(flags.FLAGS.__flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ProgressBar(Bar):\n",
    "    message = 'Loading'\n",
    "    fill = '#'\n",
    "    suffix = '%(percent).1f%% | ETA: %(eta)ds'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습결과 출력\n",
    "* Memory Network 학습 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class MemN2N(object):\n",
    "    \n",
    "    def __init__(self, config, sess):\n",
    "        self.nwords = config.nwords\n",
    "        self.max_words = config.max_words\n",
    "        self.max_sentences = config.max_sentences\n",
    "        self.init_mean = config.init_mean\n",
    "        self.init_std = config.init_std\n",
    "        self.batch_size = config.batch_size\n",
    "        self.nepoch = config.nepoch\n",
    "        self.anneal_epoch = config.anneal_epoch\n",
    "        self.nhop = config.nhop\n",
    "        self.edim = config.edim\n",
    "        self.mem_size = config.mem_size\n",
    "        self.max_grad_norm = config.max_grad_norm\n",
    "        \n",
    "        self.lin_start = config.lin_start\n",
    "        self.show_progress = config.show_progress\n",
    "        self.is_test = config.is_test\n",
    "\n",
    "        self.checkpoint_dir = config.checkpoint_dir\n",
    "        \n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        \n",
    "        self.query = tf.placeholder(tf.int32, [None, self.max_words], name='input')\n",
    "        self.time = tf.placeholder(tf.int32, [None, self.mem_size], name='time')\n",
    "        self.target = tf.placeholder(tf.float32, [None, self.nwords], name='target')\n",
    "        self.context = tf.placeholder(tf.int32, [None, self.mem_size, self.max_words], name='context')\n",
    "        \n",
    "        self.hid = []\n",
    "        \n",
    "        self.lr = None\n",
    "        \n",
    "        if self.lin_start:\n",
    "            self.current_lr = 0.005\n",
    "        else:\n",
    "            self.current_lr = config.init_lr\n",
    "\n",
    "        self.anneal_rate = config.anneal_rate\n",
    "        self.loss = None\n",
    "        self.optim = None\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.log_loss = []\n",
    "        self.log_perp = []\n",
    "    \n",
    "    def build_memory(self):\n",
    "        self.global_step = tf.Variable(0, name='global_step')\n",
    "        \n",
    "        zeros = tf.constant(0, tf.float32, [1, self.edim])\n",
    "        self.A_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.B_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.C_ = tf.Variable(tf.random_normal([self.nwords - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        \n",
    "        A = tf.concat([zeros, self.A_], axis=0)\n",
    "        B = tf.concat([zeros, self.B_], axis=0)\n",
    "        C = tf.concat([zeros, self.C_], axis=0)\n",
    "        \n",
    "        self.T_A_ = tf.Variable(tf.random_normal([self.mem_size - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        self.T_C_ = tf.Variable(tf.random_normal([self.mem_size - 1, self.edim], mean=self.init_mean, stddev=self.init_std))\n",
    "        \n",
    "        T_A = tf.concat([zeros, self.T_A_], axis=0)\n",
    "        T_C = tf.concat([zeros, self.T_C_], axis=0)\n",
    "        \n",
    "        A_ebd = tf.nn.embedding_lookup(A, self.context)   # [batch_size, mem_size, max_length, edim]\n",
    "        A_ebd = tf.reduce_sum(A_ebd, axis=2)              # [batch_size, mem_size, edim]\n",
    "        T_A_ebd = tf.nn.embedding_lookup(T_A, self.time)  # [batch_size, mem_size, edim]\n",
    "        A_in = tf.add(A_ebd, T_A_ebd)                     # [batch_size, mem_size, edim]\n",
    "        \n",
    "        C_ebd = tf.nn.embedding_lookup(C, self.context)   # [batch_size, mem_size, max_length, edim]\n",
    "        C_ebd = tf.reduce_sum(C_ebd, axis=2)              # [batch_size, mem_size, edim]\n",
    "        T_C_ebd = tf.nn.embedding_lookup(T_C, self.time)  # [batch_size, mem_size, edim]\n",
    "        C_in = tf.add(C_ebd, T_C_ebd)                     # [batch_size, mem_size, edim]\n",
    "        \n",
    "        query_ebd = tf.nn.embedding_lookup(B, self.query) # [batch_size, max_length, edim]\n",
    "        query_ebd = tf.reduce_sum(query_ebd, axis=1)      # [batch_size, edim]\n",
    "        self.hid.append(query_ebd)\n",
    "        \n",
    "        for h in range(self.nhop):\n",
    "            q3dim = tf.reshape(self.hid[-1], [-1, 1, self.edim]) # [batch_size, edim] ==> [batch_size, 1, edim]\n",
    "            p3dim = tf.matmul(q3dim, A_in, transpose_b=True)     # [batch_size, 1, edim] X [batch_size, edim, mem_size]\n",
    "            p2dim = tf.reshape(p3dim, [-1, self.mem_size])       # [batch_size, mem_size]\n",
    "            \n",
    "            # If linear start, remove softmax layers\n",
    "            if self.lin_start:\n",
    "                p = p2dim\n",
    "            else:\n",
    "                p = tf.nn.softmax(p2dim)\n",
    "            \n",
    "            p3dim = tf.reshape(p, [-1, 1, self.mem_size]) # [batch_size, 1, mem_size]\n",
    "            o3dim = tf.matmul(p3dim, C_in)                # [batch_size, 1, mem_size] X [batch_size, mem_size, edim]\n",
    "            o2dim = tf.reshape(o3dim, [-1, self.edim])    # [batch_size, edim]\n",
    "            \n",
    "            a = tf.add(o2dim, self.hid[-1]) # [batch_size, edim]\n",
    "            self.hid.append(a)              # [input, a_1, a_2, ..., a_nhop]\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.build_memory()\n",
    "        \n",
    "        self.W = tf.Variable(tf.random_normal([self.edim, self.nwords], mean=self.init_mean, stddev=self.init_std))\n",
    "        a_hat = tf.matmul(self.hid[-1], self.W)\n",
    "        \n",
    "        self.hypothesis = tf.nn.softmax(a_hat)\n",
    "\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=a_hat, labels=self.target)\n",
    "        \n",
    "        self.lr = tf.Variable(self.current_lr)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        \n",
    "        params = [self.A_, self.B_, self.C_, self.T_A_, self.T_C_, self.W]\n",
    "        grads_and_vars = self.opt.compute_gradients(self.loss, params)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], self.max_grad_norm), gv[1]) for gv in grads_and_vars]\n",
    "        \n",
    "        inc = self.global_step.assign_add(1)\n",
    "        with tf.control_dependencies([inc]):\n",
    "            self.optim = self.opt.apply_gradients(clipped_grads_and_vars)\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    def train(self, train_stories, train_questions):\n",
    "        N = int(math.ceil(len(train_questions) / self.batch_size))\n",
    "        cost = 0\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "        \n",
    "        for idx in range(N):\n",
    "            \n",
    "            if self.show_progress:\n",
    "                bar.next()\n",
    "            \n",
    "            if idx == N - 1:\n",
    "                iterations = len(train_questions) - (N - 1) * self.batch_size\n",
    "            else:\n",
    "                iterations = self.batch_size\n",
    "            \n",
    "            query = np.ndarray([iterations, self.max_words], dtype=np.int32)\n",
    "            time = np.zeros([iterations, self.mem_size], dtype=np.int32)\n",
    "            target = np.zeros([iterations, self.nwords], dtype=np.float32)\n",
    "            context = np.ndarray([iterations, self.mem_size, self.max_words], dtype=np.int32)\n",
    "            \n",
    "            for b in range(iterations):\n",
    "                m = idx * self.batch_size + b\n",
    "                \n",
    "                curr_q = train_questions[m]\n",
    "                q_text = curr_q['question']\n",
    "                story_ind = curr_q['story_index']\n",
    "                sent_ind = curr_q['sentence_index']\n",
    "                answer = curr_q['answer'][0]\n",
    "                \n",
    "                curr_s = train_stories[story_ind]\n",
    "                curr_c = curr_s[:sent_ind + 1]\n",
    "\n",
    "                if len(curr_c) >= self.mem_size:\n",
    "                    curr_c = curr_c[-self.mem_size:]\n",
    "                    \n",
    "                    for t in range(self.mem_size):\n",
    "                        time[b, t].fill(t)\n",
    "                else:\n",
    "                    \n",
    "                    for t in range(len(curr_c)):\n",
    "                        time[b, t].fill(t)\n",
    "                    \n",
    "                    while len(curr_c) < self.mem_size:\n",
    "                        curr_c.append([0.] * self.max_words)\n",
    "                \n",
    "                query[b, :] = q_text\n",
    "                target[b, answer] = 1\n",
    "                context[b, :, :] = curr_c\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim, self.loss, self.global_step],\n",
    "                                               feed_dict={self.query: query, self.time: time,\n",
    "                                                          self.target: target, self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar.finish()\n",
    "        \n",
    "        return cost / len(train_questions)\n",
    "    \n",
    "    \n",
    "    def test(self, test_stories, test_questions, label='Test'):\n",
    "        N = int(math.ceil(len(test_questions) / self.batch_size))\n",
    "        cost = 0\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "        \n",
    "        for idx in range(N):\n",
    "            \n",
    "            if self.show_progress:\n",
    "                bar.next()\n",
    "            \n",
    "            if idx == N - 1:\n",
    "                iterations = len(test_questions) - (N - 1) * self.batch_size\n",
    "            else:\n",
    "                iterations = self.batch_size\n",
    "            \n",
    "            query = np.ndarray([iterations, self.max_words], dtype=np.int32)\n",
    "            time = np.zeros([iterations, self.mem_size], dtype=np.int32)\n",
    "            target = np.zeros([iterations, self.nwords], dtype=np.float32)\n",
    "            context = np.ndarray([iterations, self.mem_size, self.max_words], dtype=np.int32)\n",
    "            \n",
    "            for b in range(iterations):\n",
    "                m = idx * self.batch_size + b\n",
    "                \n",
    "                curr_q = test_questions[m]\n",
    "                q_text = curr_q['question']\n",
    "                story_ind = curr_q['story_index']\n",
    "                sent_ind = curr_q['sentence_index']\n",
    "                answer = curr_q['answer'][0]\n",
    "                \n",
    "                curr_s = test_stories[story_ind]\n",
    "                curr_c = curr_s[:sent_ind + 1]\n",
    "                \n",
    "                if len(curr_c) >= self.mem_size:\n",
    "                    curr_c = curr_c[-self.mem_size:]\n",
    "                    \n",
    "                    for t in range(self.mem_size):\n",
    "                        time[b, t].fill(t)\n",
    "                else:\n",
    "                    \n",
    "                    for t in range(len(curr_c)):\n",
    "                        time[b, t].fill(t)\n",
    "                    \n",
    "                    while len(curr_c) < self.mem_size:\n",
    "                        curr_c.append([0.] * self.max_words)\n",
    "                \n",
    "                query[b, :] = q_text\n",
    "                target[b, answer] = 1\n",
    "                context[b, :, :] = curr_c\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim, self.loss, self.global_step],\n",
    "                                               feed_dict={self.query: query, self.time: time,\n",
    "                                                          self.target: target, self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "        \n",
    "        if self.show_progress:\n",
    "            bar.finish()\n",
    "        \n",
    "        return cost / len(test_questions)\n",
    "    \n",
    "    \n",
    "    def run(self, train_stories, train_questions, test_stories, test_questions):\n",
    "        if not self.is_test:# add not\n",
    "\n",
    "            for idx in range(self.nepoch):\n",
    "                train_loss = np.sum(self.train(train_stories, train_questions))\n",
    "                test_loss = np.sum(self.test(test_stories, test_questions, label='Validation'))\n",
    "                \n",
    "                self.log_loss.append([train_loss, test_loss])\n",
    "                \n",
    "                state = {\n",
    "                    'loss': train_loss,\n",
    "                    'epoch': idx,\n",
    "                    'learning_rate': self.current_lr,\n",
    "                    'validation_loss': test_loss\n",
    "                }\n",
    "                \n",
    "                print(state)\n",
    "                \n",
    "                \n",
    "                # learning rate annealing\n",
    "                if (not idx == 0) and (idx % self.anneal_epoch == 0):\n",
    "                    self.current_lr = self.current_lr * self.anneal_rate\n",
    "                    self.lr.assign(self.current_lr).eval()\n",
    "            \n",
    "                # If validation loss stops decreasing, insert softmax layers\n",
    "                if idx == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    if self.log_loss[idx][1] > self.log_loss[idx - 1][1]:\n",
    "                        self.lin_start = False\n",
    "\n",
    "                if idx % 10 == 0:\n",
    "                    self.saver.save(self.sess,\n",
    "                                    os.path.join(self.checkpoint_dir, \"MemN2N.model\"),\n",
    "                                    global_step=self.step.astype(int))\n",
    "        else:\n",
    "            self.load()\n",
    "            \n",
    "            valid_loss = np.sum(self.test(train_stories, train_questions, label='Validation'))\n",
    "            test_loss = np.sum(self.test(test_stories, test_questions, label='Test'))\n",
    "            \n",
    "            state = {\n",
    "                'validation_loss': valid_loss,\n",
    "                'test_loss': test_loss\n",
    "            }\n",
    "            \n",
    "            print(state)\n",
    "\n",
    "\n",
    "    def predict(self, test_stories, test_questions):\n",
    "        self.load()\n",
    "\n",
    "        num_instances = len(test_questions)\n",
    "\n",
    "        query = np.ndarray([num_instances, self.max_words], dtype=np.int32)\n",
    "        time = np.zeros([num_instances, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([num_instances, self.nwords], dtype=np.float32)\n",
    "        context = np.ndarray([num_instances, self.mem_size, self.max_words], dtype=np.int32)\n",
    "\n",
    "        for b in range(num_instances):\n",
    "            \n",
    "            curr_q = test_questions[b]\n",
    "            q_text = curr_q['question']\n",
    "            story_ind = curr_q['story_index']\n",
    "            sent_ind = curr_q['sentence_index']\n",
    "            answer = curr_q['answer'][0]\n",
    "            \n",
    "            curr_s = test_stories[story_ind]\n",
    "            curr_c = curr_s[:sent_ind + 1]\n",
    "            \n",
    "            if len(curr_c) >= self.mem_size:\n",
    "                curr_c = curr_c[-self.mem_size:]\n",
    "                \n",
    "                for t in range(self.mem_size):\n",
    "                    time[b, t].fill(t)\n",
    "            else:\n",
    "                \n",
    "                for t in range(len(curr_c)):\n",
    "                    time[b, t].fill(t)\n",
    "                \n",
    "                while len(curr_c) < self.mem_size:\n",
    "                    curr_c.append([0.] * self.max_words)\n",
    "            \n",
    "            query[b, :] = q_text\n",
    "            target[b, answer] = 1\n",
    "            context[b, :, :] = curr_c\n",
    "\n",
    "        predictions = self.sess.run(self.hypothesis, feed_dict={self.query: query, self.time: time, self.context: context})\n",
    "\n",
    "        return predictions, target\n",
    "\n",
    "\n",
    "        \n",
    "    def load(self):\n",
    "        print(' [*] Reading checkpoints...')\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\" [!] No checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8274202346801758, 'epoch': 0, 'learning_rate': 0.01, 'validation_loss': 2.8142929077148438}\n",
      "{'loss': 2.8012189865112305, 'epoch': 1, 'learning_rate': 0.01, 'validation_loss': 2.7881791591644287}\n",
      "{'loss': 2.7751531600952148, 'epoch': 2, 'learning_rate': 0.01, 'validation_loss': 2.7621212005615234}\n",
      "{'loss': 2.7490639686584473, 'epoch': 3, 'learning_rate': 0.01, 'validation_loss': 2.7359616756439209}\n",
      "{'loss': 2.7227959632873535, 'epoch': 4, 'learning_rate': 0.01, 'validation_loss': 2.7095475196838379}\n",
      "{'loss': 2.696197509765625, 'epoch': 5, 'learning_rate': 0.01, 'validation_loss': 2.6827273368835449}\n",
      "{'loss': 2.6691179275512695, 'epoch': 6, 'learning_rate': 0.01, 'validation_loss': 2.6553516387939453}\n",
      "{'loss': 2.6414093971252441, 'epoch': 7, 'learning_rate': 0.01, 'validation_loss': 2.6272730827331543}\n",
      "{'loss': 2.6129240989685059, 'epoch': 8, 'learning_rate': 0.01, 'validation_loss': 2.5983443260192871}\n",
      "{'loss': 2.5835156440734863, 'epoch': 9, 'learning_rate': 0.01, 'validation_loss': 2.5684194564819336}\n",
      "{'loss': 2.5530381202697754, 'epoch': 10, 'learning_rate': 0.01, 'validation_loss': 2.5373530387878418}\n",
      "{'loss': 2.5213465690612793, 'epoch': 11, 'learning_rate': 0.01, 'validation_loss': 2.5050005912780762}\n",
      "{'loss': 2.4882979393005371, 'epoch': 12, 'learning_rate': 0.01, 'validation_loss': 2.4712204933166504}\n",
      "{'loss': 2.4537508487701416, 'epoch': 13, 'learning_rate': 0.01, 'validation_loss': 2.4358720779418945}\n",
      "{'loss': 2.4175667762756348, 'epoch': 14, 'learning_rate': 0.01, 'validation_loss': 2.3988189697265625}\n",
      "{'loss': 2.3796119689941406, 'epoch': 15, 'learning_rate': 0.01, 'validation_loss': 2.3599300384521484}\n",
      "{'loss': 2.3397579193115234, 'epoch': 16, 'learning_rate': 0.01, 'validation_loss': 2.3190813064575195}\n",
      "{'loss': 2.2978858947753906, 'epoch': 17, 'learning_rate': 0.01, 'validation_loss': 2.276158332824707}\n",
      "{'loss': 2.2538862228393555, 'epoch': 18, 'learning_rate': 0.01, 'validation_loss': 2.2310585975646973}\n",
      "{'loss': 2.2076644897460938, 'epoch': 19, 'learning_rate': 0.01, 'validation_loss': 2.1836955547332764}\n",
      "{'loss': 2.1591436862945557, 'epoch': 20, 'learning_rate': 0.01, 'validation_loss': 2.1340024471282959}\n",
      "{'loss': 2.1082675457000732, 'epoch': 21, 'learning_rate': 0.01, 'validation_loss': 2.0819356441497803}\n",
      "{'loss': 2.0550060272216797, 'epoch': 22, 'learning_rate': 0.01, 'validation_loss': 2.0274801254272461}\n",
      "{'loss': 1.9993603229522705, 'epoch': 23, 'learning_rate': 0.01, 'validation_loss': 1.9706525802612305}\n",
      "{'loss': 1.9413648843765259, 'epoch': 24, 'learning_rate': 0.01, 'validation_loss': 1.911507248878479}\n",
      "{'loss': 1.8810930252075195, 'epoch': 25, 'learning_rate': 0.01, 'validation_loss': 1.85013747215271}\n",
      "{'loss': 1.8186593055725098, 'epoch': 26, 'learning_rate': 0.005, 'validation_loss': 1.8026993274688721}\n",
      "{'loss': 1.7866165637969971, 'epoch': 27, 'learning_rate': 0.005, 'validation_loss': 1.7704144716262817}\n",
      "{'loss': 1.7540960311889648, 'epoch': 28, 'learning_rate': 0.005, 'validation_loss': 1.7376645803451538}\n",
      "{'loss': 1.7211236953735352, 'epoch': 29, 'learning_rate': 0.005, 'validation_loss': 1.7044768333435059}\n",
      "{'loss': 1.6877279281616211, 'epoch': 30, 'learning_rate': 0.005, 'validation_loss': 1.6708806753158569}\n",
      "{'loss': 1.6539392471313477, 'epoch': 31, 'learning_rate': 0.005, 'validation_loss': 1.6369074583053589}\n",
      "{'loss': 1.6197896003723145, 'epoch': 32, 'learning_rate': 0.005, 'validation_loss': 1.6025898456573486}\n",
      "{'loss': 1.5853123664855957, 'epoch': 33, 'learning_rate': 0.005, 'validation_loss': 1.5679614543914795}\n",
      "{'loss': 1.5505417585372925, 'epoch': 34, 'learning_rate': 0.005, 'validation_loss': 1.5330572128295898}\n",
      "{'loss': 1.515512228012085, 'epoch': 35, 'learning_rate': 0.005, 'validation_loss': 1.4979112148284912}\n",
      "{'loss': 1.4802584648132324, 'epoch': 36, 'learning_rate': 0.005, 'validation_loss': 1.4625582695007324}\n",
      "{'loss': 1.4448142051696777, 'epoch': 37, 'learning_rate': 0.005, 'validation_loss': 1.4270305633544922}\n",
      "{'loss': 1.409211277961731, 'epoch': 38, 'learning_rate': 0.005, 'validation_loss': 1.3913602828979492}\n",
      "{'loss': 1.3734807968139648, 'epoch': 39, 'learning_rate': 0.005, 'validation_loss': 1.3555762767791748}\n",
      "{'loss': 1.3376498222351074, 'epoch': 40, 'learning_rate': 0.005, 'validation_loss': 1.3197048902511597}\n",
      "{'loss': 1.3017439842224121, 'epoch': 41, 'learning_rate': 0.005, 'validation_loss': 1.2837696075439453}\n",
      "{'loss': 1.2657840251922607, 'epoch': 42, 'learning_rate': 0.005, 'validation_loss': 1.2477898597717285}\n",
      "{'loss': 1.2297887802124023, 'epoch': 43, 'learning_rate': 0.005, 'validation_loss': 1.211782693862915}\n",
      "{'loss': 1.1937730312347412, 'epoch': 44, 'learning_rate': 0.005, 'validation_loss': 1.1757615804672241}\n",
      "{'loss': 1.1577498912811279, 'epoch': 45, 'learning_rate': 0.005, 'validation_loss': 1.1397393941879272}\n",
      "{'loss': 1.1217316389083862, 'epoch': 46, 'learning_rate': 0.005, 'validation_loss': 1.1037284135818481}\n",
      "{'loss': 1.0857322216033936, 'epoch': 47, 'learning_rate': 0.005, 'validation_loss': 1.0677456855773926}\n",
      "{'loss': 1.0497722625732422, 'epoch': 48, 'learning_rate': 0.005, 'validation_loss': 1.0318160057067871}\n",
      "{'loss': 1.0138825178146362, 'epoch': 49, 'learning_rate': 0.005, 'validation_loss': 0.99597853422164917}\n",
      "{'loss': 0.97811269760131836, 'epoch': 50, 'learning_rate': 0.005, 'validation_loss': 0.96029567718505859}\n",
      "{'loss': 0.94254004955291748, 'epoch': 51, 'learning_rate': 0.0025, 'validation_loss': 0.93369460105895996}\n",
      "{'loss': 0.92487096786499023, 'epoch': 52, 'learning_rate': 0.0025, 'validation_loss': 0.91607123613357544}\n",
      "{'loss': 0.90729808807373047, 'epoch': 53, 'learning_rate': 0.0025, 'validation_loss': 0.89855420589447021}\n",
      "{'loss': 0.88984286785125732, 'epoch': 54, 'learning_rate': 0.0025, 'validation_loss': 0.88116753101348877}\n",
      "{'loss': 0.87253165245056152, 'epoch': 55, 'learning_rate': 0.0025, 'validation_loss': 0.86393916606903076}\n",
      "{'loss': 0.85539352893829346, 'epoch': 56, 'learning_rate': 0.0025, 'validation_loss': 0.84689956903457642}\n",
      "{'loss': 0.83846151828765869, 'epoch': 57, 'learning_rate': 0.0025, 'validation_loss': 0.83008384704589844}\n",
      "{'loss': 0.82177138328552246, 'epoch': 58, 'learning_rate': 0.0025, 'validation_loss': 0.81352913379669189}\n",
      "{'loss': 0.80536174774169922, 'epoch': 59, 'learning_rate': 0.0025, 'validation_loss': 0.79727476835250854}\n",
      "{'loss': 0.78927338123321533, 'epoch': 60, 'learning_rate': 0.0025, 'validation_loss': 0.78136259317398071}\n",
      "{'loss': 0.77354764938354492, 'epoch': 61, 'learning_rate': 0.0025, 'validation_loss': 0.76583385467529297}\n",
      "{'loss': 0.75822603702545166, 'epoch': 62, 'learning_rate': 0.0025, 'validation_loss': 0.75072914361953735}\n",
      "{'loss': 0.74334776401519775, 'epoch': 63, 'learning_rate': 0.0025, 'validation_loss': 0.73608648777008057}\n",
      "{'loss': 0.72894912958145142, 'epoch': 64, 'learning_rate': 0.0025, 'validation_loss': 0.72193992137908936}\n",
      "{'loss': 0.71506178379058838, 'epoch': 65, 'learning_rate': 0.0025, 'validation_loss': 0.70831847190856934}\n",
      "{'loss': 0.70171165466308594, 'epoch': 66, 'learning_rate': 0.0025, 'validation_loss': 0.6952444314956665}\n",
      "{'loss': 0.68891769647598267, 'epoch': 67, 'learning_rate': 0.0025, 'validation_loss': 0.68273329734802246}\n",
      "{'loss': 0.67669177055358887, 'epoch': 68, 'learning_rate': 0.0025, 'validation_loss': 0.67079329490661621}\n",
      "{'loss': 0.66503798961639404, 'epoch': 69, 'learning_rate': 0.0025, 'validation_loss': 0.65942525863647461}\n",
      "{'loss': 0.65395379066467285, 'epoch': 70, 'learning_rate': 0.0025, 'validation_loss': 0.64862257242202759}\n",
      "{'loss': 0.64342957735061646, 'epoch': 71, 'learning_rate': 0.0025, 'validation_loss': 0.63837313652038574}\n",
      "{'loss': 0.63345074653625488, 'epoch': 72, 'learning_rate': 0.0025, 'validation_loss': 0.62865984439849854}\n",
      "{'loss': 0.62399804592132568, 'epoch': 73, 'learning_rate': 0.0025, 'validation_loss': 0.61946189403533936}\n",
      "{'loss': 0.61504840850830078, 'epoch': 74, 'learning_rate': 0.0025, 'validation_loss': 0.61075425148010254}\n",
      "{'loss': 0.60657644271850586, 'epoch': 75, 'learning_rate': 0.0025, 'validation_loss': 0.60251152515411377}\n",
      "{'loss': 0.59855610132217407, 'epoch': 76, 'learning_rate': 0.00125, 'validation_loss': 0.59662461280822754}\n",
      "{'loss': 0.59471940994262695, 'epoch': 77, 'learning_rate': 0.00125, 'validation_loss': 0.59283912181854248}\n",
      "{'loss': 0.59098410606384277, 'epoch': 78, 'learning_rate': 0.00125, 'validation_loss': 0.58915352821350098}\n",
      "{'loss': 0.58734732866287231, 'epoch': 79, 'learning_rate': 0.00125, 'validation_loss': 0.58556449413299561}\n",
      "{'loss': 0.58380520343780518, 'epoch': 80, 'learning_rate': 0.00125, 'validation_loss': 0.58206886053085327}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.58035480976104736, 'epoch': 81, 'learning_rate': 0.00125, 'validation_loss': 0.57866322994232178}\n",
      "{'loss': 0.57699275016784668, 'epoch': 82, 'learning_rate': 0.00125, 'validation_loss': 0.57534366846084595}\n",
      "{'loss': 0.57371580600738525, 'epoch': 83, 'learning_rate': 0.00125, 'validation_loss': 0.57210826873779297}\n",
      "{'loss': 0.57052075862884521, 'epoch': 84, 'learning_rate': 0.00125, 'validation_loss': 0.56895279884338379}\n",
      "{'loss': 0.56740456819534302, 'epoch': 85, 'learning_rate': 0.00125, 'validation_loss': 0.56587505340576172}\n",
      "{'loss': 0.56436431407928467, 'epoch': 86, 'learning_rate': 0.00125, 'validation_loss': 0.56287145614624023}\n",
      "{'loss': 0.56139695644378662, 'epoch': 87, 'learning_rate': 0.00125, 'validation_loss': 0.55993950366973877}\n",
      "{'loss': 0.55849945545196533, 'epoch': 88, 'learning_rate': 0.00125, 'validation_loss': 0.55707597732543945}\n",
      "{'loss': 0.55566906929016113, 'epoch': 89, 'learning_rate': 0.00125, 'validation_loss': 0.55427825450897217}\n",
      "{'loss': 0.55290329456329346, 'epoch': 90, 'learning_rate': 0.00125, 'validation_loss': 0.55154377222061157}\n",
      "{'loss': 0.55019950866699219, 'epoch': 91, 'learning_rate': 0.00125, 'validation_loss': 0.54886960983276367}\n",
      "{'loss': 0.547554612159729, 'epoch': 92, 'learning_rate': 0.00125, 'validation_loss': 0.54625385999679565}\n",
      "{'loss': 0.54496663808822632, 'epoch': 93, 'learning_rate': 0.00125, 'validation_loss': 0.54369300603866577}\n",
      "{'loss': 0.54243314266204834, 'epoch': 94, 'learning_rate': 0.00125, 'validation_loss': 0.54118573665618896}\n",
      "{'loss': 0.5399513840675354, 'epoch': 95, 'learning_rate': 0.00125, 'validation_loss': 0.53872919082641602}\n",
      "{'loss': 0.53751933574676514, 'epoch': 96, 'learning_rate': 0.00125, 'validation_loss': 0.53632134199142456}\n",
      "{'loss': 0.53513479232788086, 'epoch': 97, 'learning_rate': 0.00125, 'validation_loss': 0.53395974636077881}\n",
      "{'loss': 0.53279560804367065, 'epoch': 98, 'learning_rate': 0.00125, 'validation_loss': 0.53164267539978027}\n",
      "{'loss': 0.53049993515014648, 'epoch': 99, 'learning_rate': 0.00125, 'validation_loss': 0.52936780452728271}\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/MemN2N.model-182\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = MemN2N(FLAGS, sess)\n",
    "    model.build_model()\n",
    "\n",
    "    if FLAGS.is_test:\n",
    "        model.run(valid_stories, valid_questions, test_stories, test_questions)\n",
    "    else:\n",
    "        model.run(train_stories, train_questions, valid_stories, valid_questions)\n",
    "        \n",
    "    predictions, target = model.predict(train_stories, train_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story:\n",
      "[['john', 'travelled', 'to', 'the', 'hallway'],\n",
      " ['김수상', 'journeyed', 'to', 'the', '판교'],\n",
      " ['daniel', 'went', 'back', 'to', 'the', '판교'],\n",
      " ['john', 'moved', 'to', 'the', 'bedroom']]\n",
      "\n",
      "Question:\n",
      "['where', 'is', '김수상']\n",
      "\n",
      "Prediction:\n",
      "['판교']\n",
      "\n",
      "Answer:\n",
      "['판교']\n",
      "\n",
      "Correct:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = 1\n",
    "\n",
    "depad_data(train_stories, train_questions)\n",
    "\n",
    "question = train_questions[index]['question']\n",
    "answer = train_questions[index]['answer']\n",
    "story_index = train_questions[index]['story_index']\n",
    "sentence_index = train_questions[index]['sentence_index']\n",
    "\n",
    "story = train_stories[story_index][:sentence_index + 1]\n",
    "\n",
    "story = [list(map(idx2word.get, sentence)) for sentence in story]\n",
    "question = list(map(idx2word.get, question))\n",
    "prediction = [idx2word[np.argmax(predictions[index])]]\n",
    "answer = list(map(idx2word.get, answer))\n",
    "\n",
    "print('Story:')\n",
    "pp.pprint(story)\n",
    "print('\\nQuestion:')\n",
    "pp.pprint(question)\n",
    "print('\\nPrediction:')\n",
    "pp.pprint(prediction)\n",
    "print('\\nAnswer:')\n",
    "pp.pprint(answer)\n",
    "print('\\nCorrect:')\n",
    "pp.pprint(prediction == answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
